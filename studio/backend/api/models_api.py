"""
è®¾è®¡é™¢ (Studio) - AI æ¨¡å‹ç®¡ç† API

ä» GitHub Models API åŠ¨æ€è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œå¸¦æœ¬åœ°ç¼“å­˜ã€‚
ä½ çš„ GitHub PAT (Copilot Pro+) å†³å®šäº†ä½ èƒ½ç”¨å“ªäº›æ¨¡å‹ã€‚

GitHub Models API ç«¯ç‚¹:
  GET https://models.inference.ai.azure.com/models
  Authorization: Bearer <GITHUB_TOKEN>
"""
import asyncio
import logging
import re
import time
from typing import List, Optional, Dict, Any

import httpx
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from studio.backend.core.config import settings
from studio.backend.core.model_capabilities import capability_cache
from studio.backend.models import ModelCapabilityOverride
from studio.backend.services.copilot_auth import copilot_auth

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/studio-api/models", tags=["AI Models"])


# ==================== æ¨¡å‹å®šä¹‰ ====================

class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯ (ä» GitHub Models API æ˜ å°„)"""
    id: str = Field(description="æ¨¡å‹ IDï¼Œç”¨äº API è°ƒç”¨ (Copilot æ¨¡å‹ä»¥ 'copilot:' å‰ç¼€)")
    name: str = Field(description="æ¨¡å‹æ˜¾ç¤ºåç§°")
    publisher: str = Field("", description="å‘å¸ƒè€…/å‚å•† (å¦‚ OpenAI, Anthropic)")
    registry: str = Field("", description="æ³¨å†Œæº (å¦‚ azure-openai, github)")
    description: str = ""
    summary: str = ""
    category: str = Field("discussion", description="discussion / implementation / both")
    max_input_tokens: int = 0
    max_output_tokens: int = 4096
    supports_vision: bool = False
    supports_tools: bool = False
    supports_json_output: bool = False
    is_reasoning: bool = Field(False, description="æ¨ç†æ¨¡å‹éœ€è¦ max_completion_tokens æ›¿ä»£ max_tokens")
    api_backend: str = Field("models", description="API åç«¯: models (GitHub Models) / copilot (Copilot API)")
    pricing_tier: str = Field("free", description="å®šä»·: free(å…è´¹) / premium(æ¶ˆè€—é«˜çº§è¯·æ±‚)")
    premium_multiplier: float = Field(0, description="ä»˜è´¹ç”¨æˆ·æ¯æ¬¡æ¶ˆè€—é«˜çº§è¯·æ±‚æ•° (0=å…è´¹, 1=1æ¬¡, 0.33=â…“æ¬¡)")
    free_multiplier: Optional[float] = Field(None, description="å…è´¹ç”¨æˆ·æ¯æ¬¡æ¶ˆè€—é«˜çº§è¯·æ±‚æ•° (None=éœ€è®¢é˜…, 1=å¯ç”¨)")
    is_deprecated: bool = Field(False, description="æ˜¯å¦å³å°†å¼ƒç”¨")
    pricing_note: str = Field("", description="å®šä»·/å¼ƒç”¨è¯´æ˜")
    task: str = Field("", description="æ¨¡å‹ä»»åŠ¡ç±»å‹ (chat-completion, etc)")
    is_custom: bool = Field(False, description="æ˜¯å¦æ¥è‡ª DB è¡¥å……æ¨¡å‹ (ç”¨äºå…¨å±€å¼€å…³è¿‡æ»¤)")
    model_family: str = Field("", description="äºŒçº§åˆ†ç±»/å‚å•†æ— (å¦‚ OpenAI, DeepSeek, MiniMax ç­‰)")
    provider_slug: str = Field("", description="æä¾›å•†æ ‡è¯† (github/copilot/deepseek ç­‰)")
    provider_icon: str = Field("", description="æä¾›å•†å›¾æ ‡")
    # åŸå§‹æ•°æ®ä¿ç•™
    raw_capabilities: Dict[str, Any] = Field(default_factory=dict)


# ==================== æ¨¡å‹ç¼“å­˜ ====================

class ModelCache:
    """æ¨¡å‹åˆ—è¡¨ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚éƒ½è°ƒ GitHub API"""

    def __init__(self, ttl_seconds: int = 600):
        self.ttl = ttl_seconds  # é»˜è®¤ç¼“å­˜ 10 åˆ†é’Ÿ
        self._models: List[ModelInfo] = []
        self._last_fetch: float = 0
        self._lock = asyncio.Lock()
        self._fetch_error: Optional[str] = None

    @property
    def is_expired(self) -> bool:
        return time.time() - self._last_fetch > self.ttl

    @property
    def models(self) -> List[ModelInfo]:
        return self._models

    @property
    def last_error(self) -> Optional[str]:
        return self._fetch_error

    async def get_models(self, force_refresh: bool = False) -> List[ModelInfo]:
        """è·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¿‡æœŸåˆ™è‡ªåŠ¨åˆ·æ–°"""
        if not force_refresh and not self.is_expired and self._models:
            return self._models

        async with self._lock:
            # åŒæ£€é” â€” å…¶ä»–åç¨‹å¯èƒ½å·²åˆ·æ–°
            if not force_refresh and not self.is_expired and self._models:
                return self._models

            try:
                self._models = await _fetch_github_models()
                self._last_fetch = time.time()
                self._fetch_error = None
                logger.info(f"âœ… ä» GitHub Models API è·å–åˆ° {len(self._models)} ä¸ªæ¨¡å‹")
            except Exception as e:
                self._fetch_error = str(e)
                logger.error(f"âŒ è·å– GitHub æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
                # å¦‚æœæœ‰æ—§ç¼“å­˜å°±ç»§ç»­ç”¨ï¼Œä¸ç½®ç©º
                if not self._models:
                    raise

        return self._models


# å…¨å±€å•ä¾‹
_model_cache = ModelCache(ttl_seconds=600)  # ç¼“å­˜ 10 åˆ†é’Ÿ, æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®è§¦å‘ force_refresh


# ==================== GitHub Models API è°ƒç”¨ ====================

# å·²çŸ¥çš„å¼ºä»£ç å®ç°èƒ½åŠ›çš„æ¨¡å‹å…³é”®å­— (ç”¨äºè‡ªåŠ¨åˆ†ç±»)
_STRONG_CODE_KEYWORDS = {
    "gpt-4o", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
    "o1", "o1-mini", "o3", "o3-mini", "o4-mini",
    "claude-sonnet-4", "claude-3.5-sonnet", "claude-3-opus",
    "deepseek-chat", "deepseek-v3", "deepseek-r1",
    "codestral",
    "mistral-large",
}

# model_family â†’ æ˜¾ç¤ºåæ˜ å°„
_FAMILY_DISPLAY = {
    "openai": "OpenAI",
    "anthropic": "Anthropic",
    "meta": "Meta",
    "mistralai": "Mistral AI",
    "mistral-ai": "Mistral AI",
    "mistral": "Mistral AI",
    "deepseek": "DeepSeek",
    "google": "Google",
    "microsoft": "Microsoft",
    "cohere": "Cohere",
    "ai21 labs": "AI21 Labs",
    "xai": "xAI",
    "unknown": "Other",
}

# tags â†’ èƒ½åŠ›æ˜ å°„
_VISION_TAGS = {"multimodal", "vision", "image"}
_TOOLS_TAGS = {"agents", "tools", "function-calling"}
_REASONING_TAGS = {"reasoning"}

# å·²çŸ¥çš„æ¨ç†æ¨¡å‹ (éœ€è¦ max_completion_tokens æ›¿ä»£ max_tokens, ä¸æ”¯æŒ system prompt)
_REASONING_MODEL_PATTERNS = {"o1", "o3", "o3-mini", "o4-mini"}


# ==================== å®šä»·ä¿¡æ¯ ====================

# Copilot API æ¨¡å‹å®šä»·å€ç‡è¡¨ (åŸºäº GitHub Copilot å®˜æ–¹å®šä»·)
# ä¸¤åˆ—: paid = ä»˜è´¹ç”¨æˆ·å€ç‡ (0=è®¡åˆ’å†…å…è´¹, >0=æ¶ˆè€—é«˜çº§è¯·æ±‚)
#        free = å…è´¹ç”¨æˆ·å€ç‡ (1=å¯ç”¨æ¶ˆè€—1æ¬¡é«˜çº§è¯·æ±‚, None=éœ€è®¢é˜…)
# æ¥æº: https://docs.github.com/en/copilot/concepts/billing/copilot-requests#model-multipliers
# æœ€åæ›´æ–°: 2026-02-16 (å¯é€šè¿‡è®¾ç½®é¡µã€Œåˆ·æ–°å®šä»·ã€æŒ‰é’®ä»å®˜æ–¹æ–‡æ¡£åŒæ­¥)
_COPILOT_PREMIUM_COST: Dict[str, Dict[str, Any]] = {
    # OpenAI â€” å…è´¹åŒ…å«æ¨¡å‹ (paid plans)
    "gpt-4o": {"paid": 0, "free": 1},
    "gpt-4o-mini": {"paid": 0, "free": 1},
    "gpt-4": {"paid": 0, "free": 1},
    "gpt-4.1": {"paid": 0, "free": 1},
    "gpt-5-mini": {"paid": 0, "free": 1},
    "raptor-mini": {"paid": 0, "free": 1},
    # OpenAI â€” é«˜çº§æ¨¡å‹
    "gpt-4.1-mini": {"paid": 0.25, "free": 1},
    "gpt-4.1-nano": {"paid": 0, "free": 1},
    "gpt-5": {"paid": 1.0, "free": None},
    "gpt-5-codex": {"paid": 1.0, "free": None},
    "gpt-5.1": {"paid": 1.0, "free": None},
    "gpt-5.1-codex": {"paid": 1.0, "free": None},
    "gpt-5.1-codex-max": {"paid": 1.0, "free": None},
    "gpt-5.1-codex-mini": {"paid": 0.33, "free": 1},
    "gpt-5.2": {"paid": 1.0, "free": None},
    "gpt-5.2-codex": {"paid": 1.0, "free": None},
    "gpt-5.3-codex": {"paid": 1.0, "free": None},
    "o1": {"paid": 1.0, "free": None},
    "o1-mini": {"paid": 0.5, "free": 1},
    "o3": {"paid": 1.0, "free": None},
    "o3-mini": {"paid": 0.33, "free": 1},
    "o4-mini": {"paid": 0.33, "free": 1},
    # Anthropic
    "claude-sonnet-4": {"paid": 1.0, "free": 1},
    "claude-sonnet-4-20250514": {"paid": 1.0, "free": 1},
    "claude-sonnet-4.5": {"paid": 1.0, "free": None},
    "claude-opus-41": {"paid": 10.0, "free": None},
    "claude-opus-4.5": {"paid": 3.0, "free": None},
    "claude-opus-4.6": {"paid": 3.0, "free": None},
    "claude-opus-4.6-fast": {"paid": 9.0, "free": None},
    "claude-haiku-4.5": {"paid": 0.33, "free": 1},
    "claude-3.5-sonnet": {"paid": 0, "free": 1},
    "claude-3.7-sonnet": {"paid": 1.0, "free": 1},
    # Google
    "gemini-2.0-flash": {"paid": 0.25, "free": 1},
    "gemini-2.5-pro": {"paid": 1.0, "free": None},
    "gemini-3-flash-preview": {"paid": 0.33, "free": 1},
    "gemini-3-pro-preview": {"paid": 1.0, "free": None},
    # xAI
    "grok-3": {"paid": 1.0, "free": None},
    "grok-code-fast-1": {"paid": 0.25, "free": 1},
}

# ç¡¬ç¼–ç é»˜è®¤å€¼å‰¯æœ¬ (ç”¨äº DB diff æ¯”è¾ƒ)
_COPILOT_PREMIUM_COST_DEFAULTS: Dict[str, Dict[str, Any]] = dict(_COPILOT_PREMIUM_COST)

# å·²çŸ¥å³å°†å¼ƒç”¨ / å·²æœ‰æ›´æ–°ç‰ˆæœ¬çš„æ¨¡å‹
_DEPRECATED_MODELS: Dict[str, str] = {
    "claude-3.5-sonnet": "å»ºè®®å‡çº§åˆ° Claude Sonnet 4",
}

# åœ¨çº¿ token ä¸Šé™æ¥æº (ç¤¾åŒºç»´æŠ¤ï¼Œä¼˜å…ˆç”¨äºæ ¡å‡†é¢„è®¾å€¼)
_CONTEXT_LIMITS_SOURCE_URL = "https://raw.githubusercontent.com/taylorwilsdon/llm-context-limits/main/README.md"


async def load_pricing_overrides_from_db():
    """å¯åŠ¨æ—¶ä» DB åŠ è½½å®šä»·è¦†ç›–; è‹¥æœ‰è®°å½•åˆ™å®Œæ•´æ›¿æ¢è¿è¡Œæ—¶å®šä»·è¡¨"""
    global _COPILOT_PREMIUM_COST
    from sqlalchemy import select
    from studio.backend.core.database import async_session_maker
    try:
        async with async_session_maker() as db:
            result = await db.execute(
                select(ModelCapabilityOverride).where(
                    ModelCapabilityOverride.premium_paid.isnot(None)
                )
            )
            rows = result.scalars().all()
            if not rows:
                return
            # DB æœ‰å®šä»·æ•°æ® â†’ åˆå¹¶åˆ°ç¡¬ç¼–ç ä¸Š (åªæ›´æ–°/æ–°å¢, ä¸åˆ é™¤)
            db_pricing: Dict[str, Dict[str, Any]] = {}
            for r in rows:
                free_val = r.premium_free if r.premium_free != -1 else None
                db_pricing[r.model_name] = {
                    "paid": r.premium_paid,
                    "free": free_val,
                }
            _COPILOT_PREMIUM_COST.update(db_pricing)
            logger.info(f"âœ… ä» DB åŠ è½½äº† {len(rows)} æ¡å®šä»·è¦†ç›–, è¿è¡Œæ—¶å®šä»·è¡¨å…± {len(_COPILOT_PREMIUM_COST)} æ¡")
    except Exception as e:
        logger.warning(f"åŠ è½½å®šä»·è¦†ç›–å¤±è´¥: {e}")


def _normalize_model_key(name: str) -> str:
    key = (name or "").strip().lower()
    key = key.replace("**", "")
    key = key.replace("`", "")
    key = re.sub(r"\s+", "-", key)
    key = key.replace(":", "-")
    key = key.replace("_", "-")
    return key


def _parse_token_value(raw: str) -> int:
    if not raw:
        return 0
    s = raw.strip().lower()
    if "unknown" in s or "unclear" in s or s == "-":
        return 0
    s = s.replace("tokens", "").replace("token", "").strip()
    m = re.search(r"([0-9][0-9,]*(?:\.[0-9]+)?)\s*(k|m)?", s)
    if not m:
        return 0
    value = float(m.group(1).replace(",", ""))
    unit = m.group(2)
    if unit == "k":
        value *= 1000
    elif unit == "m":
        value *= 1000000
    return int(value)


def _parse_online_context_limits(markdown_text: str) -> Dict[str, tuple[int, int]]:
    """ä» llm-context-limits README çš„ Markdown è¡¨æ ¼è§£æ token ä¸Šé™"""
    limits: Dict[str, tuple[int, int]] = {}
    lines = markdown_text.splitlines()
    for line in lines:
        line = line.strip()
        if not line.startswith("|"):
            continue
        if set(line.replace("|", "").strip()) <= {":", "-", " "}:
            continue
        parts = [p.strip() for p in line.strip("|").split("|")]
        if len(parts) < 3:
            continue

        model_name = parts[0]
        if model_name.lower() in {"model", "model name", "endpoint"}:
            continue

        ctx = _parse_token_value(parts[1])
        out = _parse_token_value(parts[2])
        if ctx <= 0 and out <= 0:
            continue

        key = _normalize_model_key(model_name)
        if ctx <= 0:
            ctx = 128000
        if out <= 0:
            out = 4096
        limits[key] = (ctx, out)
    return limits


async def _fetch_online_context_limits() -> Dict[str, tuple[int, int]]:
    async with httpx.AsyncClient(timeout=20) as client:
        resp = await client.get(_CONTEXT_LIMITS_SOURCE_URL)
        resp.raise_for_status()
        return _parse_online_context_limits(resp.text)


def _classify_model(model_name: str, task: str, supports_tools: bool) -> str:
    """æ ¹æ®æ¨¡å‹åç§°å’Œèƒ½åŠ›è‡ªåŠ¨åˆ†ç±»: discussion / implementation / both"""
    name_lower = model_name.lower()

    # é chat ç±» â†’ ä¸å‚ä¸
    if task and "chat" not in task and "completion" not in task:
        return "discussion"

    # å·²çŸ¥å¼ºä»£ç æ¨¡å‹ â†’ both
    for keyword in _STRONG_CODE_KEYWORDS:
        if keyword in name_lower:
            return "both"

    # æ”¯æŒ tool calling / agents æ ‡ç­¾çš„é€šå¸¸ä¹Ÿèƒ½å†™ä»£ç 
    if supports_tools:
        return "both"

    return "discussion"


def _parse_model(raw: Dict[str, Any], api_backend: str = "models") -> ModelInfo:
    """
    å°† GitHub Models API è¿”å›çš„åŸå§‹æ•°æ®è§£æä¸º ModelInfo

    api_backend: "models" (GitHub Models API) æˆ– "copilot" (Copilot API)

    GitHub Models API å“åº”å­—æ®µ:
      id:             azureml://registries/.../gpt-4o/versions/2 (é•¿è·¯å¾„)
      name:           gpt-4o (çŸ­åç§°, ç”¨äº chat API è°ƒç”¨!)
      friendly_name:  OpenAI GPT-4o (æ˜¾ç¤ºå)
      model_family:   openai (å‚å•†)
      model_registry: azure-openai
      task:           chat-completion
      tags:           ["multipurpose", "multilingual", "multimodal"]
      description:    ...
      summary:        ...
    """
    # å…³é”®: name æ‰æ˜¯ chat/completions API ç”¨çš„ model å‚æ•°
    model_name = raw.get("name") or raw.get("id", "unknown")
    friendly_name = raw.get("friendly_name") or model_name
    model_family = (raw.get("model_family") or "").lower()
    registry = raw.get("model_registry") or raw.get("registry", "")
    description = raw.get("description") or ""
    summary = raw.get("summary") or ""
    task = raw.get("task") or ""
    tags = set(t.lower() for t in (raw.get("tags") or []))

    # é€šè¿‡ tags æ£€æµ‹èƒ½åŠ›
    supports_vision = bool(tags & _VISION_TAGS)
    supports_tools = bool(tags & _TOOLS_TAGS)

    # é¢å¤–: åŸºäºæ¨¡å‹åçš„ vision æ¨æ–­ (æè¿°ä¸­æ˜ç¡®è¯´æ”¯æŒ image)
    if not supports_vision:
        desc_lower = (description + summary).lower()
        if "image" in desc_lower and "input" in desc_lower:
            supports_vision = True

    # æ¨ç†æ¨¡å‹æ£€æµ‹
    is_reasoning = bool(tags & _REASONING_TAGS)
    # æ›´ç²¾ç¡®ï¼šå·²çŸ¥æ¨ç†æ¨¡å‹
    name_lower = model_name.lower()
    for pattern in _REASONING_MODEL_PATTERNS:
        if name_lower == pattern or name_lower.startswith(pattern + "-"):
            is_reasoning = True
            break

    # publisher æ˜¾ç¤ºå
    publisher = _FAMILY_DISPLAY.get(model_family, None)
    if not publisher:
        # å°è¯•ä»æ¨¡å‹åçŒœæµ‹å‚å•†å†æŸ¥è¡¨
        guessed = _guess_family(model_name)
        publisher = _FAMILY_DISPLAY.get(guessed, raw.get("publisher", model_family or model_name))

    # åˆ†ç±»
    category = _classify_model(model_name, task, supports_tools)

    # Copilot API æ¨¡å‹ ID æ·»åŠ å‰ç¼€
    model_id = f"copilot:{model_name}" if api_backend == "copilot" else model_name
    # æ˜¾ç¤ºåä¸æ·»åŠ åç«¯æ ‡è¯† (å‰ç«¯çš„åˆ†ç»„æ ‡é¢˜å·²æœ‰ â˜ï¸)
    display_name = friendly_name

    # å®šä»·ä¿¡æ¯
    # å®šä»·ä¿¡æ¯ (ä¸¤åˆ—: paid=ä»˜è´¹ç”¨æˆ·å€ç‡, free=å…è´¹ç”¨æˆ·å€ç‡)
    free_multiplier: Optional[float] = None
    if api_backend == "copilot":
        pricing_tier = "premium"
        # ç²¾ç¡®åŒ¹é… â†’ å‰ç¼€åŒ¹é… (å¤„ç†å¸¦æ—¥æœŸåç¼€çš„æ¨¡å‹å¦‚ gpt-4o-2024-08-06)
        pricing_entry = _COPILOT_PREMIUM_COST.get(model_name, None)
        if pricing_entry is None:
            # å°è¯•å‰ç¼€åŒ¹é… (å»æ‰æ—¥æœŸåç¼€)
            for known_name, entry in _COPILOT_PREMIUM_COST.items():
                if model_name.startswith(known_name):
                    pricing_entry = entry
                    break
        if pricing_entry:
            premium_multiplier = pricing_entry["paid"]
            free_multiplier = pricing_entry.get("free")
        else:
            premium_multiplier = 1.0  # æœªçŸ¥æ¨¡å‹é»˜è®¤ 1x (å¯èƒ½ä¸å‡†)
            free_multiplier = None
        if premium_multiplier == 0:
            pricing_tier = "free"
            pricing_note = "x0"
        else:
            pricing_note = f"x{premium_multiplier:g}"
    else:
        # GitHub Models API æ¨¡å‹å§‹ç»ˆå…è´¹
        pricing_tier = "free"
        premium_multiplier = 0.0
        free_multiplier = None  # GitHub Models API ä¸åŒºåˆ†
        pricing_note = "x0"

    model_is_deprecated = model_name in _DEPRECATED_MODELS
    if model_is_deprecated:
        pricing_note += f" | {_DEPRECATED_MODELS[model_name]}"

    # Token é™åˆ¶: ä¼˜å…ˆä» API åŸå§‹æ•°æ®å­¦ä¹ , å¦åˆ™æŸ¥èƒ½åŠ›ç¼“å­˜
    capability_cache.learn_from_api(model_id, raw)
    cap_input, cap_output = capability_cache.get_context_window(model_id)

    # provider ä¿¡æ¯
    provider_slug = "copilot" if api_backend == "copilot" else "github"
    provider_icon = "â˜ï¸" if api_backend == "copilot" else "ğŸ™"

    return ModelInfo(
        id=model_id,             # copilot:gpt-4o æˆ– gpt-4o
        name=display_name,       # æ˜¾ç¤ºå
        publisher=publisher,
        registry=registry,
        description=description,
        summary=summary,
        category=category,
        max_input_tokens=cap_input,
        max_output_tokens=cap_output,
        supports_vision=supports_vision,
        supports_tools=supports_tools,
        supports_json_output=False,  # API æœªè¿”å›æ­¤å­—æ®µ
        is_reasoning=is_reasoning,
        api_backend=api_backend,
        pricing_tier=pricing_tier,
        premium_multiplier=premium_multiplier,
        free_multiplier=free_multiplier,
        is_deprecated=model_is_deprecated,
        pricing_note=pricing_note,
        task=task,
        model_family=publisher,
        provider_slug=provider_slug,
        provider_icon=provider_icon,
        raw_capabilities={"tags": list(tags)},
    )


# ==================== Copilot åŠ¨æ€æ¨¡å‹å‘ç° ====================
# æ—§ç¡¬ç¼–ç æ¨¡å‹åˆ—è¡¨å·²è¿ç§»åˆ°æ•°æ®åº“ (custom_models è¡¨)
# ç§å­æ•°æ®è§ studio/backend/api/model_config.py
# ç”¨æˆ·å¯é€šè¿‡è®¾ç½®é¡µé¢å¢åˆ æ”¹è¡¥å……æ¨¡å‹

def _guess_family(model_name: str) -> str:
    """æ ¹æ®æ¨¡å‹åç§°çŒœæµ‹ model_family"""
    n = model_name.lower()
    if "claude" in n or "anthropic" in n:
        return "anthropic"
    if "gpt" in n or n.startswith("o1") or n.startswith("o3") or n.startswith("o4"):
        return "openai"
    if "gemini" in n or "google" in n:
        return "google"
    if "grok" in n or "xai" in n:
        return "xai"
    if "deepseek" in n:
        return "deepseek"
    if "mistral" in n or "codestral" in n:
        return "mistral"
    if "llama" in n or "meta" in n:
        return "meta"
    return "unknown"


def _guess_tags(model_name: str) -> list:
    """æ ¹æ®æ¨¡å‹åç§°çŒœæµ‹ tags"""
    n = model_name.lower()
    tags = []
    if any(k in n for k in ("opus", "pro", "sonnet-4", "gpt-4o", "o3", "grok-3")):
        tags.append("multimodal")
    if any(k in n for k in ("o1", "o3", "o4", "3.7", "reasoning", "think")):
        tags.append("reasoning")
    if "flash" in n or "mini" in n or "haiku" in n:
        tags.append("fast")
    tags.append("agents")
    return tags


async def _fetch_copilot_api_models() -> List[Dict[str, Any]]:
    """
    å°è¯•ä» Copilot API åŠ¨æ€è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚
    VS Code Copilot ä½¿ç”¨ GET https://api.githubcopilot.com/models è·å–æ¨¡å‹ã€‚
    è¿”å›åŸå§‹æ¨¡å‹ä¿¡æ¯åˆ—è¡¨ï¼Œå¤±è´¥è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    if not copilot_auth.is_authenticated:
        return []

    try:
        session_token = await copilot_auth.ensure_session()
        headers = {
            "Authorization": f"Bearer {session_token}",
            "Accept": "application/json",
            "editor-version": "vscode/1.96.0",
            "editor-plugin-version": "copilot-chat/0.24.0",
            "copilot-integration-id": "vscode-chat",
            "user-agent": "Studio/1.0",
        }

        async with httpx.AsyncClient(timeout=15) as client:
            resp = await client.get(
                "https://api.githubcopilot.com/models",
                headers=headers,
            )

            if resp.status_code == 200:
                data = resp.json()
                # å…¼å®¹å¤šç§è¿”å›æ ¼å¼
                if isinstance(data, list):
                    raw_list = data
                elif isinstance(data, dict):
                    raw_list = data.get("data") or data.get("models") or data.get("value") or []
                else:
                    raw_list = []

                logger.info(f"Copilot API /models è¿”å› {len(raw_list)} ä¸ªæ¨¡å‹")
                return raw_list
            else:
                logger.warning(f"Copilot /models è¿”å› {resp.status_code}: {resp.text[:200]}")
    except Exception as e:
        logger.warning(f"è·å– Copilot æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")

    return []


async def _fetch_github_models() -> List[ModelInfo]:
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨:
    1. ä» GitHub Models API åŠ¨æ€è·å– (backend="models")
    2. åˆå¹¶ DB ä¸­çš„ backend="models" è¡¥å……æ¨¡å‹ (å»é‡)
    3. å¦‚æœ Copilot å·²æˆæƒ:
       a. å…ˆå°è¯• Copilot API åŠ¨æ€è·å– (ä¼˜å…ˆ)
       b. å¤±è´¥æ—¶å›é€€åˆ° DB ä¸­çš„ backend="copilot" æ¨¡å‹
    4. åº”ç”¨ DB èƒ½åŠ›è¦†ç›–
    """
    token = settings.github_token
    if not token:
        raise RuntimeError("æœªé…ç½® GITHUB_TOKENï¼Œæ— æ³•è·å–æ¨¡å‹åˆ—è¡¨")

    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json",
    }

    models: List[ModelInfo] = []
    seen_names: set = set()

    async with httpx.AsyncClient(timeout=30) as client:
        resp = await client.get(
            f"{settings.github_models_endpoint}/models",
            headers=headers,
        )

        if resp.status_code == 200:
            data = resp.json()
            raw_list = data if isinstance(data, list) else (
                data.get("data") or data.get("models") or data.get("value") or []
            )

            for raw in raw_list:
                try:
                    model = _parse_model(raw)
                    # åªä¿ç•™ chat ç›¸å…³æ¨¡å‹
                    if model.task and "chat" not in model.task and "completion" not in model.task:
                        continue
                    models.append(model)
                    seen_names.add(model.id.lower())
                except Exception as e:
                    logger.warning(f"è§£ææ¨¡å‹æ•°æ®å¤±è´¥: {e}, raw={raw.get('name', 'unknown')}")
                    continue

            logger.info(f"GitHub Models API è¿”å› {len(raw_list)} ä¸ªåŸå§‹æ¨¡å‹, è§£æå‡º {len(models)} ä¸ª chat æ¨¡å‹")
        else:
            error_text = resp.text[:500]
            logger.warning(f"GitHub Models API è¿”å› {resp.status_code}: {error_text}")

    # ä» DB åŠ è½½è¡¥å……æ¨¡å‹ (æ›¿ä»£åŸç¡¬ç¼–ç  _COPILOT_PRO_EXTRA_MODELS)
    db_extra_models = await _load_db_custom_models("models")
    extra_count = 0
    for extra_raw in db_extra_models:
        if extra_raw["name"].lower() not in seen_names:
            try:
                model = _parse_model(extra_raw, api_backend="models")
                model.is_custom = True
                models.append(model)
                seen_names.add(model.id.lower())
                extra_count += 1
            except Exception as e:
                logger.warning(f"è§£æ DB è¡¥å……æ¨¡å‹å¤±è´¥: {e}")

    if extra_count:
        logger.info(f"ä» DB è¡¥å……äº† {extra_count} ä¸ª models åç«¯æ¨¡å‹")

    # æ·»åŠ  Copilot API ä¸“å±æ¨¡å‹ (å¦‚æœå·²æˆæƒ)
    if copilot_auth.is_authenticated:
        copilot_count = 0
        copilot_seen = set()

        # å…ˆå°è¯•ä» Copilot API åŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
        dynamic_models = await _fetch_copilot_api_models()
        _SKIP_PATTERNS = {"embedding", "text-embedding", "oswe-"}
        if dynamic_models:
            for cp_raw in dynamic_models:
                cp_name = (cp_raw.get("id") or cp_raw.get("name") or "").strip()
                if not cp_name:
                    continue
                cp_lower = cp_name.lower()
                if any(pat in cp_lower for pat in _SKIP_PATTERNS):
                    continue
                copilot_id = f"copilot:{cp_lower}"
                if copilot_id in copilot_seen:
                    continue

                # ä» Copilot API çš„ capabilities å­—æ®µæå–ä¸°å¯Œèƒ½åŠ›ä¿¡æ¯
                caps = cp_raw.get("capabilities", {})
                caps_supports = caps.get("supports", {})
                caps_limits = caps.get("limits", {})

                # æ„å»º tags: å…ˆç”¨ API èƒ½åŠ›æ•°æ®ï¼Œå†ç”¨çŒœæµ‹å…œåº•
                api_tags = set()
                if caps_supports.get("vision"):
                    api_tags.add("multimodal")
                if caps_supports.get("tool_calls"):
                    api_tags.add("agents")
                if caps_supports.get("adaptive_thinking"):
                    api_tags.add("reasoning")
                merged_tags = list(api_tags) if api_tags else _guess_tags(cp_name)

                parsed_raw = {
                    "name": cp_name,
                    "friendly_name": cp_raw.get("name") or cp_raw.get("friendly_name") or cp_name,
                    "model_family": caps.get("family") or cp_raw.get("vendor") or _guess_family(cp_name),
                    "task": caps.get("type", "chat") + "-completion",
                    "tags": merged_tags,
                    "summary": cp_raw.get("summary") or cp_raw.get("description") or "",
                    # ä¼ é€’ API åŸå§‹èƒ½åŠ›æ•°æ®ï¼Œä¾› learn_from_api ä½¿ç”¨
                    "max_input_tokens": caps_limits.get("max_prompt_tokens") or caps_limits.get("max_context_window_tokens") or 0,
                    "max_output_tokens": caps_limits.get("max_output_tokens") or 0,
                }
                try:
                    model = _parse_model(parsed_raw, api_backend="copilot")
                    models.append(model)
                    copilot_seen.add(copilot_id)
                    copilot_count += 1
                except Exception as e:
                    logger.warning(f"è§£æ Copilot åŠ¨æ€æ¨¡å‹å¤±è´¥: {e}")
            logger.info(f"Copilot API åŠ¨æ€å‘ç° {copilot_count} ä¸ªæ¨¡å‹")
        else:
            # åŠ¨æ€è·å–å¤±è´¥ï¼Œå›é€€åˆ° DB ä¸­çš„ copilot æ¨¡å‹ (æ›¿ä»£åŸç¡¬ç¼–ç åˆ—è¡¨)
            db_copilot_models = await _load_db_custom_models("copilot")
            for cp_raw in db_copilot_models:
                cp_name = cp_raw["name"].lower()
                copilot_id = f"copilot:{cp_name}"
                if copilot_id not in copilot_seen:
                    try:
                        model = _parse_model(cp_raw, api_backend="copilot")
                        model.is_custom = True
                        models.append(model)
                        copilot_seen.add(copilot_id)
                        copilot_count += 1
                    except Exception as e:
                        logger.warning(f"è§£æ DB Copilot æ¨¡å‹å¤±è´¥: {e}")
            logger.info(f"å›é€€ä½¿ç”¨ DB é…ç½®ï¼Œ{copilot_count} ä¸ª Copilot æ¨¡å‹")

        if copilot_count:
            logger.info(f"æ·»åŠ äº† {copilot_count} ä¸ª Copilot API ä¸“å±æ¨¡å‹")
    else:
        logger.info("Copilot æœªæˆæƒï¼Œè·³è¿‡ Copilot ä¸“å±æ¨¡å‹")

    # åº”ç”¨ DB èƒ½åŠ›è¦†ç›– (supports_vision, supports_tools, is_reasoning)
    await _apply_db_capability_overrides(models)

    # åŠ è½½ç¬¬ä¸‰æ–¹æä¾›å•† (openai_compatible) çš„æ¨¡å‹
    await _append_third_party_models(models, seen_names)

    # æŒ‰ publisher æ’åºï¼Œå¸¸ç”¨æ¨¡å‹é å‰
    publisher_order = {"OpenAI": 0, "Anthropic": 1, "Google": 2, "DeepSeek": 3, "Mistral AI": 4, "Meta": 5, "xAI": 6, "AI21 Labs": 7}
    models.sort(key=lambda m: (publisher_order.get(m.publisher, 99), m.api_backend, m.name))

    return models


async def _load_db_custom_models(api_backend: str) -> list:
    """ä» DB åŠ è½½æŒ‡å®šåç«¯çš„è‡ªå®šä¹‰æ¨¡å‹ (è¿”å›ä¸ _parse_model å…¼å®¹çš„ dict åˆ—è¡¨)"""
    from studio.backend.core.database import async_session_maker
    try:
        async with async_session_maker() as db:
            from studio.backend.api.model_config import get_custom_models_from_db
            return await get_custom_models_from_db(db, api_backend=api_backend)
    except Exception as e:
        logger.warning(f"ä» DB åŠ è½½ {api_backend} è‡ªå®šä¹‰æ¨¡å‹å¤±è´¥: {e}")
        return []


async def _apply_db_capability_overrides(models: List[ModelInfo]):
    """åº”ç”¨ DB ä¸­çš„èƒ½åŠ›è¦†ç›–åˆ°æ¨¡å‹åˆ—è¡¨ (boolean èƒ½åŠ›: vision, tools, reasoning)"""
    from studio.backend.core.database import async_session_maker
    try:
        async with async_session_maker() as db:
            from studio.backend.api.model_config import get_capability_overrides_map
            overrides = await get_capability_overrides_map(db)
            if not overrides:
                return

            for m in models:
                clean = m.id.removeprefix("copilot:").lower()
                ov = overrides.get(clean)
                if not ov:
                    continue
                if ov.supports_vision is not None:
                    m.supports_vision = ov.supports_vision
                if ov.supports_tools is not None:
                    m.supports_tools = ov.supports_tools
                if ov.is_reasoning is not None:
                    m.is_reasoning = ov.is_reasoning
                # token é™åˆ¶å·²é€šè¿‡ capability_cache çš„ DB override å±‚å¤„ç†
                # è¿™é‡ŒåŒæ­¥æ›´æ–° ModelInfo çš„æ˜¾ç¤ºå€¼
                if ov.max_input_tokens is not None:
                    m.max_input_tokens = ov.max_input_tokens
                if ov.max_output_tokens is not None:
                    m.max_output_tokens = ov.max_output_tokens

            logger.debug(f"åº”ç”¨äº† {len(overrides)} æ¡ DB èƒ½åŠ›è¦†ç›–")
    except Exception as e:
        logger.warning(f"åº”ç”¨ DB èƒ½åŠ›è¦†ç›–å¤±è´¥: {e}")


async def _append_third_party_models(models: List[ModelInfo], seen_names: set):
    """ä»å·²å¯ç”¨çš„ç¬¬ä¸‰æ–¹ (openai_compatible) æä¾›å•†åŠ è½½æ¨¡å‹ â€” ä¼˜å…ˆ API åŠ¨æ€å‘ç°, å›é€€åˆ°é¢„è®¾"""
    try:
        from studio.backend.api.provider_api import get_enabled_providers
        providers = await get_enabled_providers()
    except Exception as e:
        logger.warning(f"åŠ è½½ç¬¬ä¸‰æ–¹æä¾›å•†å¤±è´¥: {e}")
        return

    for prov in providers:
        if prov.provider_type != "openai_compatible":
            continue
        if not prov.api_key:
            continue  # æ—  API Key çš„æä¾›å•†è·³è¿‡

        # ä¼˜å…ˆå°è¯• API åŠ¨æ€å‘ç°æ¨¡å‹
        discovered = await _discover_provider_models(prov)
        model_list = discovered if discovered else (prov.default_models or [])
        source = "APIå‘ç°" if discovered else "é¢„è®¾"

        count = 0
        for dm in model_list:
            model_name = dm.get("name", "")
            if not model_name:
                continue
            full_id = f"{prov.slug}:{model_name}"
            if full_id.lower() in seen_names:
                continue

            friendly = dm.get("friendly_name") or dm.get("name", model_name)
            family = dm.get("model_family") or prov.name
            tags = dm.get("tags", [])
            summary = dm.get("summary", "")

            models.append(ModelInfo(
                id=full_id,
                name=friendly,
                publisher=prov.name,
                registry=prov.slug,
                description=summary,
                summary=summary,
                category="both",
                max_input_tokens=dm.get("max_input_tokens", 0),
                max_output_tokens=dm.get("max_output_tokens", 4096),
                supports_vision="multimodal" in tags or "vision" in tags,
                supports_tools="agents" in tags or "tools" in tags,
                supports_json_output="json" in tags,
                is_reasoning="reasoning" in tags,
                api_backend=prov.slug,
                pricing_tier="paid",
                premium_multiplier=0,
                free_multiplier=None,
                task="chat-completion",
                is_custom=False,
                model_family=family,
                provider_slug=prov.slug,
                provider_icon=prov.icon,
            ))
            seen_names.add(full_id.lower())
            count += 1

        logger.info(f"ç¬¬ä¸‰æ–¹æä¾›å•† {prov.name} ({prov.slug}): æ·»åŠ  {count} ä¸ªæ¨¡å‹ ({source})")


async def _discover_provider_models(prov) -> list:
    """å°è¯•ä»æä¾›å•†çš„ /models ç«¯ç‚¹åŠ¨æ€å‘ç°å¯ç”¨æ¨¡å‹"""
    base_url = prov.base_url.rstrip("/")
    try:
        async with httpx.AsyncClient(timeout=8) as client:
            resp = await client.get(
                f"{base_url}/models",
                headers={"Authorization": f"Bearer {prov.api_key}", "Accept": "application/json"},
            )
            if resp.status_code == 200:
                data = resp.json()
                raw_list = data.get("data", data) if isinstance(data, dict) else data
                if not isinstance(raw_list, list):
                    return []
                _SKIP = {"embed", "tts", "whisper", "audio", "image", "dall", "moderation",
                         "rerank", "ocr", "asr", "s2s", "mt-", "livetranslate",
                         "gui-", "tongyi-xiaomi", "z-image", "deep-search"}
                models = []
                for m in raw_list:
                    mid = m.get("id") or m.get("name", "")
                    if not mid:
                        continue
                    mid_lower = mid.lower()
                    if any(pat in mid_lower for pat in _SKIP):
                        continue
                    family, clean_name = _parse_model_family(mid, prov.name)
                    token_info = _guess_token_limits(mid)
                    models.append({
                        "name": mid,
                        "friendly_name": clean_name,
                        "model_family": family,
                        "tags": _guess_tags_from_name(mid),
                        "summary": "",
                        "max_input_tokens": token_info[0],
                        "max_output_tokens": token_info[1],
                    })
                logger.info(f"ä» {prov.slug} API åŠ¨æ€å‘ç° {len(models)} ä¸ªæ¨¡å‹")
                return models
    except Exception as e:
        logger.debug(f"ä» {prov.slug} API å‘ç°æ¨¡å‹å¤±è´¥ (å›é€€åˆ°é¢„è®¾): {e}")
    return []


def _parse_model_family(model_id: str, provider_name: str) -> tuple:
    """
    ä»æ¨¡å‹ ID è§£æäºŒçº§åˆ†ç±» (model_family) å’Œå¹²å‡€çš„æ˜¾ç¤ºåã€‚
    å¦‚ 'MiniMax/MiniMax-M2.1' â†’ ('MiniMax', 'MiniMax-M2.1')
    å¦‚ 'siliconflow/deepseek-v3.2' â†’ ('SiliconFlow', 'deepseek-v3.2')
    å¦‚ 'deepseek-r1' â†’ ('DeepSeek', 'deepseek-r1')
    å¦‚ 'qwen3-max' â†’ (provider_name, 'qwen3-max')  # å±äºæä¾›å•†è‡ªå·±
    """
    # 1. å¸¦æ–œæ çš„å‰ç¼€: 'Vendor/model-name'
    if "/" in model_id:
        prefix, rest = model_id.split("/", 1)
        family = _KNOWN_FAMILIES.get(prefix.lower(), prefix)
        return (family, rest)

    # 2. æ ¹æ®æ¨¡å‹åç§°å‰ç¼€çŒœæµ‹å‚å•†
    n = model_id.lower()
    for key, display in _KNOWN_FAMILIES.items():
        if n.startswith(key):
            return (display, model_id)

    # 3. é»˜è®¤å½’å±æä¾›å•†è‡ªèº«
    return (provider_name, model_id)


# å·²çŸ¥æ¨¡å‹æ— â†’ æ˜¾ç¤ºå (å°å†™ key)
_KNOWN_FAMILIES: Dict[str, str] = {
    "deepseek": "DeepSeek",
    "glm": "Zhipu GLM",
    "kimi": "Kimi",
    "minimax": "MiniMax",
    "siliconflow": "SiliconFlow",
    "claude": "Anthropic",
    "gpt": "OpenAI",
    "o1": "OpenAI",
    "o3": "OpenAI",
    "o4": "OpenAI",
    "gemini": "Google",
    "llama": "Meta",
    "mistral": "Mistral AI",
    "codestral": "Mistral AI",
    "qwq": "Qwen",
    "qvq": "Qwen",
    "qwen": "Qwen",
    "codeqwen": "Qwen",
}


def _guess_token_limits(model_id: str) -> tuple:
    """
    æ ¹æ®æ¨¡å‹åç§°çŒœæµ‹ (max_input_tokens, max_output_tokens)ã€‚
    è¿”å› (0, 4096) è¡¨ç¤ºæœªçŸ¥ã€‚
    """
    n = model_id.lower()
    # é•¿ä¸Šä¸‹æ–‡æ¨¡å‹
    if "1m" in n:
        return (1048576, 8192)
    # DeepSeek
    if "deepseek-v3" in n or "deepseek-r1" in n:
        return (65536, 8192)
    if "deepseek" in n:
        return (32768, 4096)
    # GLM
    if "glm-4" in n:
        return (128000, 4096)
    # Kimi
    if "kimi" in n:
        return (131072, 8192)
    # MiniMax
    if "minimax" in n:
        return (1048576, 16384)
    # Qwen 3.5
    if "qwen3.5" in n:
        return (131072, 16384)
    # Qwen 3 max/plus
    if "qwen3-max" in n or "qwen3-coder" in n:
        return (131072, 16384)
    # Qwen 3 base/open
    if "qwen3" in n:
        return (32768, 8192)
    # QwQ / QvQ reasoning
    if "qwq" in n or "qvq" in n:
        return (131072, 16384)
    # Qwen 2.5 variants
    if "qwen2.5" in n:
        return (131072, 8192)
    # Qwen general (plus/max/turbo)
    if "qwen-max" in n or "qwen-plus" in n:
        return (131072, 8192)
    if "qwen-turbo" in n or "qwen-flash" in n:
        return (131072, 8192)
    # Older qwen
    if "qwen" in n:
        return (32768, 4096)
    return (0, 4096)


# å·²çŸ¥æ”¯æŒ function calling çš„æ¨¡å‹å‰ç¼€ (å°å†™)
_KNOWN_TOOL_MODELS = {
    # Qwen ç³»åˆ— (é€šä¹‰åƒé—®) â€” qwen3 åŠä»¥ä¸Šå…¨ç³»æ”¯æŒ tools
    "qwen3", "qwen2.5", "qwen2", "qwen-max", "qwen-plus", "qwen-turbo", "qwen-long",
    "qwq",  # QwQ æ¨ç†æ¨¡å‹ä¹Ÿæ”¯æŒ tools
    # DeepSeek
    "deepseek-v3", "deepseek-v2", "deepseek-chat", "deepseek-coder",
    # GLM (æ™ºè°±)
    "glm-4", "glm-3",
    # Kimi (Moonshot)
    "kimi", "moonshot",
    # MiniMax
    "minimax",
    # Mistral
    "mistral-large", "mistral-medium", "mistral-small", "codestral",
    # Meta Llama 3+
    "llama-3", "llama-4",
    # Cohere
    "command-r", "command-a",
    # xAI
    "grok",
}

# å·²çŸ¥æ”¯æŒè§†è§‰çš„æ¨¡å‹å‰ç¼€ (éåç§°å« vl/vision çš„)
_KNOWN_VISION_MODELS = {
    "qwen2.5-vl", "qwen2-vl", "qvq",
    "glm-4v",
    "kimi-vision",
}


def _guess_tags_from_name(model_name: str) -> list:
    """æ ¹æ®æ¨¡å‹åç§°çŒœæµ‹èƒ½åŠ› tags"""
    n = model_name.lower()
    tags = []
    # å¤šæ¨¡æ€ (è§†è§‰)
    if any(k in n for k in ("vl", "vision", "multimodal", "omni")):
        tags.append("multimodal")
    elif any(n.startswith(p) for p in _KNOWN_VISION_MODELS):
        tags.append("multimodal")
    # æ¨ç†
    if any(k in n for k in ("think", "r1", "qwq", "qvq")):
        tags.append("reasoning")
    # å·¥å…·è°ƒç”¨: åç§°å…³é”®è¯ + å·²çŸ¥æ¨¡å‹æ—
    if any(k in n for k in ("tool", "agent", "function")):
        tags.append("agents")
    elif any(n.startswith(p) or n == p for p in _KNOWN_TOOL_MODELS):
        tags.append("agents")
    # ç¼–ç å¢å¼º
    if any(k in n for k in ("coder", "codestral", "codeqwen")):
        tags.append("coding")
    return tags


# ==================== æ¨¡å‹èƒ½åŠ›çŸ¥è¯†åº“ (å†…ç½®æ ¡å‡†æ•°æ®) ====================
# æ ¼å¼: {model_name_prefix: {supports_vision, supports_tools, is_reasoning}}
# ç”¨äºæ ¡å‡†ç«¯ç‚¹æ‰¹é‡å†™å…¥ DB, å‰ç¼€åŒ¹é… (gpt-4o åŒ¹é… gpt-4o-2024-08-06)
# None è¡¨ç¤ºä¸è¦†ç›–è¯¥å­—æ®µ (ä¿ç•™åŸå€¼)
_STATIC_MODEL_CAPABILITIES: Dict[str, Dict[str, Any]] = {
    # ===== OpenAI =====
    "gpt-4o": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-4o-mini": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-4": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "gpt-4-turbo": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-4.1": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-4.1-mini": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-4.1-nano": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-5": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-5-mini": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-5-codex": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "gpt-5.1": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gpt-5.1-codex": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "gpt-5.2": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "o1": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "o1-mini": {"supports_vision": False, "supports_tools": True, "is_reasoning": True},
    "o3": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "o3-mini": {"supports_vision": False, "supports_tools": True, "is_reasoning": True},
    "o4-mini": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    # ===== Anthropic =====
    "claude-3.5-sonnet": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "claude-3.7-sonnet": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "claude-sonnet-4": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "claude-sonnet-4.5": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "claude-opus-4": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "claude-opus-4.5": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "claude-opus-4.6": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "claude-haiku-4.5": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    # ===== Google =====
    "gemini-2.0-flash": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gemini-2.5-pro": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    "gemini-3-flash": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "gemini-3-pro": {"supports_vision": True, "supports_tools": True, "is_reasoning": True},
    # ===== xAI =====
    "grok-3": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "grok-code-fast": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    # ===== DeepSeek =====
    "deepseek-r1": {"supports_vision": False, "supports_tools": True, "is_reasoning": True},
    "deepseek-v3": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "deepseek-chat": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "deepseek-coder": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    # ===== Qwen (é€šä¹‰åƒé—®) =====
    "qwen3": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen3-max": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen3-plus": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen3-coder": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen3.5": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen2.5": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen2.5-vl": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "qwen-max": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen-plus": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen-turbo": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwen-long": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "qwq": {"supports_vision": False, "supports_tools": True, "is_reasoning": True},
    "qvq": {"supports_vision": True, "supports_tools": False, "is_reasoning": True},
    # ===== GLM (æ™ºè°±) =====
    "glm-4": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "glm-4v": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    # ===== Kimi (Moonshot) =====
    "kimi": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "moonshot": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    # ===== MiniMax =====
    "minimax": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    # ===== Mistral =====
    "mistral-large": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "codestral": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    # ===== Meta =====
    "llama-3": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "llama-4-scout": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    "llama-4-maverick": {"supports_vision": True, "supports_tools": True, "is_reasoning": False},
    # ===== Microsoft =====
    "phi-4": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    # ===== Cohere =====
    "command-r": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
    "command-a": {"supports_vision": False, "supports_tools": True, "is_reasoning": False},
}


# ==================== Routes ====================

@router.get("", response_model=List[ModelInfo])
async def list_models(
    category: Optional[str] = Query(None, description="ç­›é€‰: discussion / implementation / both"),
    vision_only: bool = Query(False, description="åªè¿”å›æ”¯æŒå›¾ç‰‡çš„æ¨¡å‹"),
    api_backend: Optional[str] = Query(None, description="ç­›é€‰ API åç«¯: models / copilot"),
    refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜"),
    custom_models: bool = Query(True, description="æ˜¯å¦åŒ…å«è¡¥å……æ¨¡å‹ (å…¨å±€å¼€å…³)"),
):
    """
    è·å–å¯ç”¨çš„ AI æ¨¡å‹åˆ—è¡¨

    æ¨¡å‹åˆ—è¡¨ä» GitHub Models API åŠ¨æ€è·å–ï¼Œä½¿ç”¨ä½ çš„ GITHUB_TOKEN é‰´æƒã€‚
    å¦‚æœå·²å®Œæˆ Copilot OAuth æˆæƒï¼Œè¿˜ä¼šåŒ…å« Copilot ä¸“å±æ¨¡å‹ï¼ˆClaude, Gemini ç­‰ï¼‰ã€‚
    Copilot æ¨¡å‹çš„ id ä»¥ "copilot:" å‰ç¼€æ ‡è¯†ï¼Œç”¨ â˜ï¸ å›¾æ ‡åœ¨å‰ç«¯å±•ç¤ºã€‚

    ç¼“å­˜ 10 åˆ†é’Ÿï¼Œå¯é€šè¿‡ refresh=true å¼ºåˆ¶åˆ·æ–°ã€‚
    """
    try:
        models = await _model_cache.get_models(force_refresh=refresh)
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

    # å§‹ç»ˆé‡æ–°åº”ç”¨ DB èƒ½åŠ›è¦†ç›–, ç¡®ä¿ç”¨æˆ·ä¿®æ”¹ç«‹å³ç”Ÿæ•ˆ
    # (ç¼“å­˜ä¸­çš„æ¨¡å‹å¯èƒ½å¸¦æœ‰è¿‡æœŸçš„è¦†ç›–å€¼)
    await _apply_db_capability_overrides(models)

    # ç­›é€‰
    result = models
    if not custom_models:
        result = [m for m in result if not m.is_custom]
    if category:
        result = [m for m in result if m.category in (category, "both")]
    if vision_only:
        result = [m for m in result if m.supports_vision]
    if api_backend:
        result = [m for m in result if m.api_backend == api_backend]

    return result


@router.get("/cache-status")
async def get_cache_status():
    """è·å–æ¨¡å‹ç¼“å­˜çŠ¶æ€"""
    return {
        "cached_count": len(_model_cache.models),
        "is_expired": _model_cache.is_expired,
        "last_error": _model_cache.last_error,
        "ttl_seconds": _model_cache.ttl,
        "seconds_since_fetch": int(time.time() - _model_cache._last_fetch) if _model_cache._last_fetch else None,
        "copilot_authenticated": copilot_auth.is_authenticated,
    }


@router.post("/refresh")
async def refresh_models():
    """å¼ºåˆ¶åˆ·æ–°æ¨¡å‹ç¼“å­˜"""
    try:
        models = await _model_cache.get_models(force_refresh=True)
        return {"success": True, "count": len(models)}
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"åˆ·æ–°å¤±è´¥: {str(e)}")


@router.post("/token-limits/refresh")
async def refresh_capabilities_online():
    """æ¨¡å‹èƒ½åŠ›æ ¡å‡†: è”ç½‘æŠ“å– token ä¸Šé™ + å†…ç½®çŸ¥è¯†åº“æ ¡å‡†è§†è§‰/å·¥å…·/æ¨ç†èƒ½åŠ›"""
    try:
        online_limits = await _fetch_online_context_limits()
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"åœ¨çº¿æŠ“å– token ä¸Šé™å¤±è´¥: {str(e)}")

    if not online_limits:
        raise HTTPException(status_code=503, detail="åœ¨çº¿æ¥æºæœªè§£æåˆ°å¯ç”¨ token ä¸Šé™æ•°æ®")

    models = await _model_cache.get_models(force_refresh=False)
    updated = 0
    matched: list[str] = []

    from sqlalchemy import select
    from studio.backend.core.database import async_session_maker

    async with async_session_maker() as db:
        for m in models:
            candidates = [
                _normalize_model_key(m.id.removeprefix("copilot:")),
                _normalize_model_key(m.name),
            ]

            found = None
            for key in candidates:
                if key in online_limits:
                    found = online_limits[key]
                    break
                # å‰ç¼€åŒ¹é…å…œåº•ï¼ˆå¤„ç†æ—¥æœŸåç¼€, å¦‚ gpt-4o-2024-08-06ï¼‰
                # è¦æ±‚å‰ç¼€åé¢ç´§è·Ÿåˆ†éš”ç¬¦ (-) æˆ–åˆ°ç»“å°¾, é¿å… gpt-4 åŒ¹é… gpt-4o
                import re as _re
                for remote_key, val in online_limits.items():
                    if key.startswith(remote_key) and (len(key) == len(remote_key) or key[len(remote_key)] == '-'):
                        found = val
                        break
                    if remote_key.startswith(key) and (len(remote_key) == len(key) or remote_key[len(key)] == '-'):
                        found = val
                        break
                if found:
                    break

            if not found:
                continue

            max_in, max_out = found
            clean = m.id.removeprefix("copilot:").lower()

            # æ£€æŸ¥æ˜¯å¦çœŸæ­£æœ‰å˜åŒ–
            old_in, old_out = capability_cache.get_context_window(clean)
            if old_in == max_in and old_out == max_out:
                matched.append(clean)
                continue  # å€¼ç›¸åŒ,è·³è¿‡å†™å…¥

            capability_cache._learned[clean] = (max_in, max_out)

            result = await db.execute(
                select(ModelCapabilityOverride).where(ModelCapabilityOverride.model_name == clean)
            )
            override = result.scalar_one_or_none()
            if override:
                override.max_input_tokens = max_in
                override.max_output_tokens = max_out
            else:
                db.add(ModelCapabilityOverride(
                    model_name=clean,
                    max_input_tokens=max_in,
                    max_output_tokens=max_out,
                ))

            capability_cache.set_db_override(clean, max_input=max_in, max_output=max_out)
            updated += 1
            matched.append(clean)

        # === ç¬¬äºŒæ­¥: æ ¡å‡†èƒ½åŠ› (vision/tools/reasoning) ===
        cap_updated = 0
        for m in models:
            clean = m.id.removeprefix("copilot:").lower()
            caps = _STATIC_MODEL_CAPABILITIES.get(clean)
            if not caps:
                # å‰ç¼€åŒ¹é…
                for known_key, known_caps in _STATIC_MODEL_CAPABILITIES.items():
                    if clean.startswith(known_key) and (len(clean) == len(known_key) or clean[len(known_key)] == '-'):
                        caps = known_caps
                        break
            if not caps:
                continue

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            need_update = False
            kw: Dict[str, Any] = {}
            for field in ("supports_vision", "supports_tools", "is_reasoning"):
                new_val = caps.get(field)
                if new_val is None:
                    continue
                old_val = getattr(m, field, None)
                if old_val != new_val:
                    need_update = True
                    kw[field] = new_val

            if not need_update:
                continue

            result = await db.execute(
                select(ModelCapabilityOverride).where(ModelCapabilityOverride.model_name == clean)
            )
            override = result.scalar_one_or_none()
            if override:
                for k, v in kw.items():
                    setattr(override, k, v)
            else:
                db.add(ModelCapabilityOverride(model_name=clean, **kw))
            cap_updated += 1

        await db.commit()

    return {
        "success": True,
        "source": _CONTEXT_LIMITS_SOURCE_URL,
        "online_count": len(online_limits),
        "updated_count": updated,
        "cap_updated": cap_updated,
        "matched_models": matched,
    }


@router.get("/capabilities/all")
async def get_all_capabilities():
    """è·å–æ‰€æœ‰å·²çŸ¥æ¨¡å‹çš„èƒ½åŠ›æ•°æ® (å« API å­¦ä¹  + ç¡¬ç¼–ç )"""
    known = capability_cache.get_all_known()
    result = {}
    for model_name, (max_input, max_output) in known.items():
        result[model_name] = {
            "max_input_tokens": max_input,
            "max_output_tokens": max_output,
        }
    return result


class CapabilityUpdate(BaseModel):
    max_input_tokens: Optional[int] = None
    max_output_tokens: Optional[int] = None


@router.patch("/capabilities/{model_id:path}")
async def update_model_capability(model_id: str, data: CapabilityUpdate):
    """æ‰‹åŠ¨æ›´æ–°æ¨¡å‹èƒ½åŠ›æ•°æ® (å‰ç«¯è®¾ç½®é¡µé¢è°ƒç”¨, åŒæ—¶æŒä¹…åŒ–åˆ° DB)"""
    current_in, current_out = capability_cache.get_context_window(model_id)
    clean = model_id.removeprefix("copilot:").lower()
    new_in = data.max_input_tokens or current_in
    new_out = data.max_output_tokens or current_out
    capability_cache._learned[clean] = (new_in, new_out)

    # ä¹ŸæŒä¹…åŒ–åˆ° DB
    try:
        from studio.backend.core.database import async_session_maker
        from sqlalchemy import select
        async with async_session_maker() as db:
            result = await db.execute(
                select(ModelCapabilityOverride).where(ModelCapabilityOverride.model_name == clean)
            )
            override = result.scalar_one_or_none()
            if override:
                if data.max_input_tokens is not None:
                    override.max_input_tokens = data.max_input_tokens
                if data.max_output_tokens is not None:
                    override.max_output_tokens = data.max_output_tokens
            else:
                override = ModelCapabilityOverride(
                    model_name=clean,
                    max_input_tokens=data.max_input_tokens,
                    max_output_tokens=data.max_output_tokens,
                )
                db.add(override)
            await db.commit()
            capability_cache.set_db_override(clean, max_input=override.max_input_tokens, max_output=override.max_output_tokens)
    except Exception as e:
        logger.warning(f"æŒä¹…åŒ–èƒ½åŠ›è¦†ç›–å¤±è´¥: {e}")

    return {
        "ok": True,
        "model": model_id,
        "max_input_tokens": new_in,
        "max_output_tokens": new_out,
    }


@router.get("/{model_id}", response_model=ModelInfo)
async def get_model(model_id: str):
    """è·å–æ¨¡å‹è¯¦æƒ…"""
    try:
        models = await _model_cache.get_models()
    except Exception:
        raise HTTPException(status_code=503, detail="æ¨¡å‹åˆ—è¡¨ä¸å¯ç”¨")

    for m in models:
        if m.id == model_id:
            return m
    raise HTTPException(status_code=404, detail=f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨æˆ–ä¸åœ¨ä½ çš„å¯ç”¨èŒƒå›´å†…")


# ==================== å®šä»·åˆ·æ–° ====================

_PRICING_DOC_URL = "https://docs.github.com/en/copilot/concepts/billing/copilot-requests"


async def _scrape_github_pricing() -> Dict[str, Dict[str, Any]]:
    """
    ä» GitHub å®˜æ–¹æ–‡æ¡£æŠ“å–æ¨¡å‹å®šä»·å€ç‡è¡¨ã€‚
    é¡µé¢åŒ…å«ä¸€ä¸ª HTML tableï¼Œåˆ—: Model | Multiplier for paid plans | Multiplier for Copilot Free
    è¿”å› {model_name_lower: {"paid": float, "free": float|None}}
    """
    async with httpx.AsyncClient(timeout=20) as client:
        resp = await client.get(_PRICING_DOC_URL, headers={
            "Accept": "text/html",
            "User-Agent": "Studio/1.0",
        })
        if resp.status_code != 200:
            raise RuntimeError(f"è·å–æ–‡æ¡£å¤±è´¥: HTTP {resp.status_code}")
        html = resp.text

    # è§£æ HTML ä¸­ "Model multipliers" section çš„ table
    parsed: Dict[str, Dict[str, Any]] = {}

    # æ‰¾åˆ° model-multipliers éƒ¨åˆ†
    anchor = html.find('id="model-multipliers"')
    if anchor == -1:
        raise RuntimeError("é¡µé¢ä¸­æœªæ‰¾åˆ° #model-multipliers é”šç‚¹")

    table_section = html[anchor:]
    # æ‰¾ç¬¬ä¸€ä¸ª <table>
    table_start = table_section.find("<table")
    if table_start == -1:
        raise RuntimeError("æœªæ‰¾åˆ°å®šä»·è¡¨æ ¼")

    table_end = table_section.find("</table>", table_start)
    table_html = table_section[table_start:table_end]

    # è§£æè¡Œ: <tr><th scope="row">Model</th><td>Paid multiplier</td><td>Free multiplier</td></tr>
    row_pattern = re.compile(r"<tr[^>]*>(.*?)</tr>", re.DOTALL)
    cell_pattern = re.compile(r"<(?:td|th)[^>]*>(.*?)</(?:td|th)>", re.DOTALL)

    for row_match in row_pattern.finditer(table_html):
        row_html = row_match.group(1)
        cells = cell_pattern.findall(row_html)
        if len(cells) < 2:
            continue

        model_name_raw = re.sub(r"<[^>]+>", "", cells[0]).strip()
        paid_raw = re.sub(r"<[^>]+>", "", cells[1]).strip().lower()

        # è·³è¿‡è¡¨å¤´è¡Œ
        if not model_name_raw or model_name_raw.lower() == "model":
            continue

        # è§£æä»˜è´¹å€ç‡ (å·¦åˆ—: Multiplier for paid plans)
        paid_mult = None
        if "not applicable" not in paid_raw:
            num_match = re.match(r"([\d.]+)", paid_raw)
            if num_match:
                paid_mult = float(num_match.group(1))

        if paid_mult is None:
            continue  # æ— æ³•è§£æä»˜è´¹å€ç‡ï¼Œè·³è¿‡

        # è§£æå…è´¹å€ç‡ (å³åˆ—: Multiplier for Copilot Free)
        free_mult = None
        if len(cells) >= 3:
            free_raw = re.sub(r"<[^>]+>", "", cells[2]).strip().lower()
            if "not applicable" not in free_raw:
                num_match_f = re.match(r"([\d.]+)", free_raw)
                if num_match_f:
                    free_mult = float(num_match_f.group(1))

        # æ ‡å‡†åŒ–æ¨¡å‹åï¼ˆè½¬å°å†™ï¼Œå»æ‰æ‹¬å·æ³¨é‡Šä½†ä¿ç•™æ ¸å¿ƒåï¼‰
        clean_name = _normalize_model_name(model_name_raw)
        if clean_name:
            parsed[clean_name] = {"paid": paid_mult, "free": free_mult}

    if not parsed:
        raise RuntimeError("è§£æå®šä»·è¡¨å¤±è´¥: æœªæå–åˆ°ä»»ä½•æ¨¡å‹æ•°æ®")

    return parsed


def _normalize_model_name(display_name: str) -> str:
    """
    å°† GitHub æ–‡æ¡£ä¸­çš„æ¨¡å‹æ˜¾ç¤ºåæ ‡å‡†åŒ–ä¸º API æ¨¡å‹ IDã€‚

    å¦‚: "Claude Opus 4.6 (fast mode) (preview)" â†’ "claude-opus-4.6-fast"
         "GPT-5.1-Codex-Mini" â†’ "gpt-5.1-codex-mini"
         "Gemini 3 Flash" â†’ "gemini-3-flash-preview" (å°è¯•æ˜ å°„)
    """
    name = display_name.strip()

    # æå–å¿«é€Ÿæ¨¡å¼æ ‡è®°
    is_fast = "(fast mode)" in name.lower() or "(fast)" in name.lower()
    # å»æ‰æ‹¬å·æ³¨é‡Š
    name = re.sub(r'\(.*?\)', '', name).strip()

    # æ ‡å‡†åŒ–: ç©ºæ ¼ â†’ è¿å­—ç¬¦, å°å†™
    name = re.sub(r'\s+', '-', name).strip('-').lower()

    if is_fast:
        name += "-fast"

    # ç‰¹æ®Šæ˜ å°„ (æ–‡æ¡£æ˜¾ç¤ºå â†’ API ID)
    _DOC_TO_API = {
        "grok-code-fast-1": "grok-code-fast-1",
        "raptor-mini": "raptor-mini",
        "gemini-3-flash": "gemini-3-flash-preview",
        "gemini-3-pro": "gemini-3-pro-preview",
        "gemini-2.5-pro": "gemini-2.5-pro",
        "claude-opus-4.6-fast": "claude-opus-4.6-fast",
    }
    return _DOC_TO_API.get(name, name)


def _format_pricing_change(old_entry: Dict, new_entry: Dict) -> str:
    """æ ¼å¼åŒ–å®šä»·å˜æ›´è¯´æ˜"""
    parts = []
    if old_entry["paid"] != new_entry["paid"]:
        parts.append(f"ä»˜è´¹: x{old_entry['paid']:g}â†’x{new_entry['paid']:g}")
    old_free = old_entry.get("free")
    new_free = new_entry.get("free")
    if old_free != new_free:
        old_f = f"x{old_free:g}" if old_free is not None else "éœ€è®¢é˜…"
        new_f = f"x{new_free:g}" if new_free is not None else "éœ€è®¢é˜…"
        parts.append(f"å…è´¹: {old_f}â†’{new_f}")
    return ", ".join(parts) if parts else "æ•°æ®å˜æ›´"


@router.post("/pricing/refresh")
async def refresh_pricing():
    """
    ä» GitHub å®˜æ–¹æ–‡æ¡£æŠ“å–æœ€æ–°çš„æ¨¡å‹å®šä»·å€ç‡è¡¨ï¼Œä¸å½“å‰ç¡¬ç¼–ç è¡¨å¯¹æ¯”ã€‚
    è¿”å›å·®å¼‚åˆ—è¡¨ (å«ä»˜è´¹/å…è´¹ä¸¤åˆ—)ï¼Œä¾›å‰ç«¯äºŒæ¬¡ç¡®è®¤ååº”ç”¨ã€‚
    """
    global _COPILOT_PREMIUM_COST

    try:
        scraped = await _scrape_github_pricing()
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"æŠ“å–å®˜æ–¹å®šä»·å¤±è´¥: {e}")

    # å¯¹æ¯”å·®å¼‚: åªæ£€æŸ¥å˜æ›´å’Œæ–°å¢, ä¸æŠ¥å‘Šåˆ é™¤ (æœªåŒ¹é…çš„ä¿ç•™åŸæ ·)
    changes: List[Dict[str, Any]] = []

    for model in sorted(scraped.keys()):
        old_entry = _COPILOT_PREMIUM_COST.get(model)
        new_entry = scraped[model]

        if old_entry is not None:
            if old_entry != new_entry:
                changes.append({
                    "model": model,
                    "type": "changed",
                    "old_paid": old_entry["paid"],
                    "new_paid": new_entry["paid"],
                    "old_free": old_entry.get("free"),
                    "new_free": new_entry.get("free"),
                    "note": _format_pricing_change(old_entry, new_entry),
                })
        else:
            free_note = f"å…è´¹x{new_entry['free']:g}" if new_entry.get("free") is not None else "éœ€è®¢é˜…"
            changes.append({
                "model": model,
                "type": "added",
                "old_paid": None,
                "new_paid": new_entry["paid"],
                "old_free": None,
                "new_free": new_entry.get("free"),
                "note": f"æ–°æ¨¡å‹: ä»˜è´¹x{new_entry['paid']:g}, {free_note}",
            })

    return {
        "has_changes": len(changes) > 0,
        "changes": changes,
        "scraped_count": len(scraped),
        "current_count": len(_COPILOT_PREMIUM_COST),
        "scraped": scraped,  # å®Œæ•´æ–°è¡¨ï¼Œç”¨äºåº”ç”¨
    }


class PricingApplyRequest(BaseModel):
    scraped: Dict[str, Any]  # Dict[str, {"paid": float, "free": float|None}]


@router.post("/pricing/apply")
async def apply_pricing(data: PricingApplyRequest):
    """
    åº”ç”¨æ–°çš„å®šä»·è¡¨å¹¶æŒä¹…åŒ–åˆ°æ•°æ®åº“ã€‚
    åˆå¹¶ç¡¬ç¼–ç é»˜è®¤å€¼ + DB è¦†ç›–, é‡å¯åè‡ªåŠ¨åŠ è½½ã€‚
    åŒæ—¶è§¦å‘æ¨¡å‹ç¼“å­˜åˆ·æ–°ä»¥ä½¿æ–°å®šä»·ç”Ÿæ•ˆã€‚
    """
    global _COPILOT_PREMIUM_COST
    old_count = len(_COPILOT_PREMIUM_COST)

    # å…¼å®¹å¤„ç†: æ–°æ ¼å¼ {"paid": x, "free": y} æˆ–æ—§æ ¼å¼ float
    new_pricing: Dict[str, Dict[str, Any]] = {}
    for model, entry in data.scraped.items():
        if isinstance(entry, dict):
            new_pricing[model] = entry
        else:
            new_pricing[model] = {"paid": float(entry), "free": None}

    # æŒä¹…åŒ–åˆ° DB: ä¿å­˜æ‰€æœ‰æ¡ç›® (é‡å¯åå®Œæ•´æ›¿æ¢ç¡¬ç¼–ç )
    from sqlalchemy import select
    from studio.backend.core.database import async_session_maker
    db_saved = 0
    async with async_session_maker() as db:
        for model_name, entry in new_pricing.items():
            clean = model_name.lower()
            result = await db.execute(
                select(ModelCapabilityOverride).where(ModelCapabilityOverride.model_name == clean)
            )
            override = result.scalar_one_or_none()
            paid_val = entry.get("paid")
            free_val = entry.get("free")
            # ç”¨ -1 è¡¨ç¤º free=None (éœ€è®¢é˜…), å› ä¸º DB Column æ— æ³•åŒºåˆ† null å’Œ "æœªè®¾"
            free_db = free_val if free_val is not None else -1
            if override:
                override.premium_paid = paid_val
                override.premium_free = free_db
            else:
                db.add(ModelCapabilityOverride(
                    model_name=clean,
                    premium_paid=paid_val,
                    premium_free=free_db,
                ))
            db_saved += 1
        await db.commit()

    _COPILOT_PREMIUM_COST.update(new_pricing)  # åˆå¹¶: åªæ›´æ–°/æ–°å¢, ä¸åˆ é™¤æœªåŒ¹é…çš„
    # åˆ·æ–°æ¨¡å‹ç¼“å­˜ä½¿æ–°å®šä»·ç”Ÿæ•ˆ
    try:
        await _model_cache.get_models(force_refresh=True)
    except Exception:
        pass

    return {
        "ok": True,
        "old_count": old_count,
        "new_count": len(_COPILOT_PREMIUM_COST),
        "db_saved": db_saved,
        "message": "å®šä»·è¡¨å·²æ›´æ–°å¹¶æŒä¹…åŒ–åˆ°æ•°æ®åº“",
    }


@router.get("/pricing/current")
async def get_current_pricing():
    """è·å–å½“å‰ä½¿ç”¨çš„å®šä»·è¡¨ (ä¸¤åˆ—: paid + free)"""
    return {
        "pricing": _COPILOT_PREMIUM_COST,
        "count": len(_COPILOT_PREMIUM_COST),
        "source": "hardcoded + runtime overrides",
    }
