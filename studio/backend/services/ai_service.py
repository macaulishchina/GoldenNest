"""
è®¾è®¡é™¢ (Studio) - AI å¯¹è¯æœåŠ¡

æ”¯æŒåŒ API åç«¯:
  1. GitHub Models API (models.inference.ai.azure.com) - ç”¨ PAT ç›´æ¥è°ƒç”¨
  2. GitHub Copilot API (api.githubcopilot.com) - ç”¨ OAuth æˆæƒ, æ”¯æŒæ‰€æœ‰ Copilot æ¨¡å‹ (å« Claude)

æ¨¡å‹ ID ä»¥ "copilot:" å‰ç¼€åŒºåˆ†åç«¯:
  - "gpt-4o"          â†’ GitHub Models API
  - "copilot:gpt-4o"  â†’ Copilot API

SSE äº‹ä»¶åè®® (chat_stream yield ç»“æ„åŒ– dict):
  {"type": "content",    "content": "..."}            - æ–‡æœ¬å†…å®¹
  {"type": "thinking",   "content": "..."}            - æ¨ç†è¿‡ç¨‹ (reasoning models)
  {"type": "tool_call",  "tool_call": {...}}          - AI è¯·æ±‚è°ƒç”¨å·¥å…·
  {"type": "tool_result", "tool_call_id": "...", ...} - å·¥å…·æ‰§è¡Œç»“æœ
  {"type": "tool_error", "tool_call_id": "...", ...}  - å·¥å…·æ‰§è¡Œå¤±è´¥
  {"type": "usage",      "usage": {...}}              - token ä½¿ç”¨ç»Ÿè®¡
  {"type": "error",      "error": "..."}              - é”™è¯¯ä¿¡æ¯
"""
import asyncio
import base64
import hashlib
import json
import logging
import mimetypes
import os
import re
import time
import uuid
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable, Awaitable, Set

from dataclasses import dataclass
from typing import Literal

import httpx

from studio.backend.core.config import settings
from studio.backend.core.model_capabilities import capability_cache
from studio.backend.core.token_utils import estimate_tokens, estimate_messages_tokens, truncate_text
from studio.backend.services.copilot_auth import copilot_auth, COPILOT_CHAT_URL

logger = logging.getLogger(__name__)

# ==================== Copilot è®¡è´¹ä¼šè¯ç®¡ç† ====================
# VS Code çš„ Copilot è®¡è´¹æœºåˆ¶:
#   - æ¯æ¡ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆä¸€ä¸ªæ–°çš„ x-request-id
#   - è¯¥æ¶ˆæ¯å†…çš„å·¥å…·è°ƒç”¨è½®æ¬¡å¤ç”¨åŒä¸€ä¸ª x-request-id
#   - GitHub åç«¯é€šè¿‡ x-request-id å°†åŒä¸€è½®å¯¹è¯çš„å¤šæ¬¡ API è°ƒç”¨å½’é›†ä¸ºä¸€æ¬¡ premium request
#   - vscode-sessionid å’Œ vscode-machineid ç”¨äºè®¡è´¹ä¸Šä¸‹æ–‡å…³è”
#
# æ­£ç¡®å®ç°: æ¯æ¬¡è°ƒç”¨ chat_stream() ç”Ÿæˆæ–° request_idï¼Œ
# è¯¥æ¬¡è°ƒç”¨å†…æ‰€æœ‰å·¥å…·è½®æ¬¡å¤ç”¨åŒä¸€ä¸ª IDã€‚

# åº”ç”¨å®ä¾‹çº§ session ID (å¯åŠ¨æ—¶ç”Ÿæˆä¸€æ¬¡, ç¨³å®šç”¨äºè®¡è´¹å…³è”)
_STUDIO_SESSION_ID = str(uuid.uuid4()) + str(int(time.time() * 1000))
_STUDIO_MACHINE_ID = hashlib.sha256(
    f"{os.uname().nodename}-{settings.data_path}-studio".encode()
).hexdigest()


def new_request_id() -> str:
    """æ¯æ¬¡ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆæ–°çš„ request_id

    å·¥å…·è°ƒç”¨è½®æ¬¡å†…å¤ç”¨åŒä¸€ä¸ª ID (chat_stream å†…éƒ¨å¤„ç†)ã€‚
    è¿™æ · GitHub åç«¯å°†åŒä¸€ä¸ª request_id çš„å¤šæ¬¡ API è°ƒç”¨å½’é›†ä¸ºä¸€æ¬¡ premium requestã€‚
    """
    rid = str(uuid.uuid4())
    logger.info(f"æ–°æ¶ˆæ¯ request_id: {rid[:8]}...")
    return rid

def _parse_error_metadata(status_code: int, error_text: str, model: str) -> Dict[str, Any]:
    """ä» API é”™è¯¯å“åº”ä¸­æå–ç»“æ„åŒ–å…ƒæ•°æ® (ä¾›å‰ç«¯å±•ç¤ºå’Œæ¨¡å‹èƒ½åŠ›å­¦ä¹ )"""
    meta: Dict[str, Any] = {"status_code": status_code, "model": model}

    lower = error_text.lower()

    # é€Ÿç‡é™åˆ¶
    if status_code == 429 or "rate limit" in lower:
        meta["error_type"] = "rate_limit"
        m = re.search(r'Rate limit of (\d+) per (\d+)s', error_text, re.I)
        if m:
            meta["rate_limit"] = f"{m.group(1)} per {m.group(2)}s"
            meta["rate_limit_count"] = int(m.group(1))
            meta["rate_limit_seconds"] = int(m.group(2))
        m = re.search(r'(\d+) per (\d+) (second|minute|hour)', error_text, re.I)
        if m and "rate_limit" not in meta:
            unit_map = {"second": 1, "minute": 60, "hour": 3600}
            secs = int(m.group(2)) * unit_map.get(m.group(3).lower(), 1)
            meta["rate_limit"] = f"{m.group(1)} per {secs}s"
            meta["rate_limit_count"] = int(m.group(1))
            meta["rate_limit_seconds"] = secs
        m = re.search(r'wait\s+(\d+)\s*seconds?', error_text, re.I)
        if m:
            meta["wait_seconds"] = int(m.group(1))

    # ä¸Šä¸‹æ–‡/token è¶…é™
    elif "context length" in lower or "too large" in lower or "max_tokens" in lower:
        meta["error_type"] = "context_overflow"
        m = re.search(r'maximum context length.*?(\d{3,})', error_text, re.I)
        if m:
            meta["max_context_tokens"] = int(m.group(1))
        m = re.search(r'Max size:\s*(\d+)\s*tokens', error_text, re.I)
        if m:
            meta["max_context_tokens"] = int(m.group(1))
        m = re.search(r'requested\s+(\d+)\s*tokens', error_text, re.I)
        if m:
            meta["requested_tokens"] = int(m.group(1))

    # è®¤è¯é”™è¯¯
    elif status_code in (401, 403):
        meta["error_type"] = "auth_error"

    else:
        meta["error_type"] = "unknown"

    return meta


# GitHub Models API ç«¯ç‚¹
GITHUB_MODELS_URL = settings.github_models_endpoint

# Copilot API å‰ç¼€æ ‡è¯†
COPILOT_PREFIX = "copilot:"

# æ¨ç†æ¨¡å‹éœ€è¦ç‰¹æ®Šå‚æ•°å¤„ç† (max_completion_tokens æ›¿ä»£ max_tokens, ä¸æ”¯æŒ system æ¶ˆæ¯)
_REASONING_MODEL_PREFIXES = ("o1", "o3", "o4")


def _is_reasoning_model(model: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹"""
    name = model.lower().removeprefix(COPILOT_PREFIX.lower())
    for prefix in _REASONING_MODEL_PREFIXES:
        if name == prefix or name.startswith(prefix + "-"):
            return True
    return False


def _is_copilot_model(model: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Copilot API åç«¯"""
    return model.startswith(COPILOT_PREFIX)


def _get_actual_model_name(model: str) -> str:
    """æå–å®é™…æ¨¡å‹å (å»æ‰ copilot: å‰ç¼€)"""
    if model.startswith(COPILOT_PREFIX):
        return model[len(COPILOT_PREFIX):]
    return model


async def _get_copilot_headers(request_id: str = "") -> Dict[str, str]:
    """è·å– Copilot API è¯·æ±‚å¤´

    Args:
        request_id: è®¡è´¹å½’é›† IDã€‚
                    åŒä¸€ä¸ª request_id ä¸‹çš„æ‰€æœ‰ API è°ƒç”¨ (åŒ…æ‹¬å·¥å…·è°ƒç”¨è½®æ¬¡)
                    ä¼šè¢« GitHub åç«¯å½’é›†ä¸ºä¸€æ¬¡ premium requestã€‚
                    æ¯æ¡ç”¨æˆ·æ¶ˆæ¯åº”ä½¿ç”¨æ–°çš„ request_id (new_request_id())ã€‚
    """
    session_token = await copilot_auth.ensure_session()
    return {
        "Authorization": f"Bearer {session_token}",
        "Content-Type": "application/json",
        "editor-version": "vscode/1.96.0",
        "editor-plugin-version": "copilot-chat/0.24.0",
        "copilot-integration-id": "vscode-chat",
        "openai-intent": "conversation-panel",
        "user-agent": "Studio/1.0",
        # è®¡è´¹å½’é›†å¤´ â€” åŒä¸€ request_id çš„å·¥å…·è°ƒç”¨è½®æ¬¡åˆå¹¶è®¡è´¹
        "x-request-id": request_id or str(uuid.uuid4()),
        "vscode-sessionid": _STUDIO_SESSION_ID,
        "vscode-machineid": _STUDIO_MACHINE_ID,
    }


def _get_models_headers() -> Dict[str, str]:
    """è·å– GitHub Models API è¯·æ±‚å¤´"""
    return {
        "Authorization": f"Bearer {settings.github_token}",
        "Content-Type": "application/json",
    }


# ==================== å¤šæœåŠ¡å•†è·¯ç”± ====================

@dataclass
class ProviderInfo:
    """è§£æåçš„æä¾›å•†ä¿¡æ¯"""
    provider_type: Literal["github_models", "copilot", "openai_compatible"]
    slug: str              # æä¾›å•†æ ‡è¯†
    actual_model: str      # å®é™…æ¨¡å‹å (å»æ‰å‰ç¼€)
    base_url: str          # API åŸºåœ°å€
    api_key: str           # API Key (openai_compatible)
    icon: str              # å›¾æ ‡
    name: str              # æä¾›å•†åç§°


# ç¼“å­˜å·²è§£æçš„æä¾›å•† (é¿å…æ¯æ¬¡ DB æŸ¥è¯¢)
_provider_cache: Dict[str, ProviderInfo] = {}
_provider_cache_ts: float = 0
_PROVIDER_CACHE_TTL = 60  # 60 ç§’ç¼“å­˜


async def _resolve_provider(model_id: str) -> ProviderInfo:
    """
    æ ¹æ®æ¨¡å‹ ID è§£ææä¾›å•†ä¿¡æ¯

    æ¨¡å‹ ID æ ¼å¼:
      - "gpt-4o"              â†’ GitHub Models (æ— å‰ç¼€)
      - "copilot:gpt-4o"      â†’ Copilot API
      - "deepseek:deepseek-chat" â†’ ç¬¬ä¸‰æ–¹æä¾›å•†
    """
    global _provider_cache, _provider_cache_ts

    # å†…ç½®: copilot: å‰ç¼€
    if model_id.startswith(COPILOT_PREFIX):
        actual = model_id[len(COPILOT_PREFIX):]
        return ProviderInfo(
            provider_type="copilot",
            slug="copilot",
            actual_model=actual,
            base_url=COPILOT_CHAT_URL,
            api_key="",
            icon="â˜ï¸",
            name="Copilot",
        )

    # æ£€æŸ¥æ˜¯å¦æœ‰ slug: å‰ç¼€ (ç¬¬ä¸‰æ–¹æä¾›å•†)
    if ":" in model_id:
        slug, actual = model_id.split(":", 1)

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{slug}:{actual}"
        now = time.time()
        if cache_key in _provider_cache and (now - _provider_cache_ts) < _PROVIDER_CACHE_TTL:
            cached = _provider_cache[cache_key]
            return ProviderInfo(
                provider_type=cached.provider_type,
                slug=cached.slug,
                actual_model=actual,
                base_url=cached.base_url,
                api_key=cached.api_key,
                icon=cached.icon,
                name=cached.name,
            )

        # æŸ¥æ•°æ®åº“
        from studio.backend.api.provider_api import get_provider_by_slug
        provider = await get_provider_by_slug(slug)
        if provider and provider.enabled:
            info = ProviderInfo(
                provider_type=provider.provider_type,
                slug=provider.slug,
                actual_model=actual,
                base_url=provider.base_url,
                api_key=provider.api_key or "",
                icon=provider.icon,
                name=provider.name,
            )
            _provider_cache[cache_key] = info
            _provider_cache_ts = now
            return info
        else:
            logger.warning(f"æä¾›å•† '{slug}' ä¸å­˜åœ¨æˆ–æœªå¯ç”¨, å›é€€åˆ° GitHub Models")

    # é»˜è®¤: GitHub Models
    return ProviderInfo(
        provider_type="github_models",
        slug="github",
        actual_model=model_id,
        base_url=GITHUB_MODELS_URL,
        api_key=settings.github_token or "",
        icon="ğŸ™",
        name="GitHub Models",
    )


def invalidate_provider_cache():
    """æ¸…é™¤æä¾›å•†ç¼“å­˜ (é…ç½®å˜æ›´åè°ƒç”¨)"""
    global _provider_cache, _provider_cache_ts
    _provider_cache.clear()
    _provider_cache_ts = 0


def _build_api_messages(
    messages: List[Dict[str, Any]],
    system_prompt: str,
    is_reasoning: bool,
) -> List[Dict[str, Any]]:
    """æ„å»º API æ¶ˆæ¯åˆ—è¡¨"""
    api_messages = []

    if system_prompt:
        if is_reasoning:
            api_messages.append({
                "role": "user",
                "content": f"[System Instructions]\n{system_prompt}",
            })
        else:
            api_messages.append({"role": "system", "content": system_prompt})

    for msg in messages:
        role = msg["role"]
        content = msg.get("content", "")
        images = msg.get("images", [])

        # Tool role messages (tool execution results)
        if role == "tool":
            api_messages.append({
                "role": "tool",
                "tool_call_id": msg.get("tool_call_id", ""),
                "content": msg.get("content", ""),
            })
            continue

        # Assistant messages with tool_calls
        if role == "assistant" and "tool_calls" in msg:
            entry: Dict[str, Any] = {"role": "assistant"}
            if content:
                entry["content"] = content
            else:
                entry["content"] = None
            entry["tool_calls"] = msg["tool_calls"]
            api_messages.append(entry)
            continue

        if images and role == "user":
            content_parts = []
            if content:
                content_parts.append({"type": "text", "text": content})
            for img in images:
                content_parts.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{img['mime_type']};base64,{img['base64']}"
                    },
                })
            api_messages.append({"role": role, "content": content_parts})
        else:
            api_messages.append({"role": role, "content": content})

    return api_messages


# ==================== ä¼ªé€ æ£€æµ‹ ====================

import re as _re

# æ£€æµ‹ AI åœ¨æ–‡æœ¬ä¸­ä¼ªé€ å·¥å…·æ‰§è¡Œç»“æœçš„æ¨¡å¼
# ç­–ç•¥: å®½æ¾åŒ¹é… â€” å®å¯è¯¯åˆ¤ä¹Ÿä¸æ”¾è¿‡ä¼ªé€  (è¯¯åˆ¤æœ€å¤šå¤šè°ƒç”¨ä¸€æ¬¡æ¨¡å‹)
_FABRICATION_PATTERNS = [
    # 1. ä»»ä½•å½¢å¼çš„ "å·²/é€šè¿‡/å·¥å…· + æ‰§è¡Œ" ç»„åˆ
    _re.compile(r"(å·²|å·²ç»|æˆ‘å·²|æˆ‘å·²ç»|å·²é€šè¿‡|é€šè¿‡).{0,15}(æ‰§è¡Œ|è¿è¡Œ|è°ƒç”¨).{0,20}(å‘½ä»¤|æŒ‡ä»¤|rm|touch|mkdir|cp|mv|cat|ls|git|docker|pip|npm|cd|echo|python|curl|wget|chmod|chown|kill|bash|sh|find|grep|sed|awk)", _re.IGNORECASE),
    # 2. ç›´æ¥è¯´ "æ‰§è¡Œäº†/è¿è¡Œäº†" + å‘½ä»¤ç›¸å…³
    _re.compile(r"(æ‰§è¡Œäº†|è¿è¡Œäº†|å·²è¿è¡Œ|å·²æ‰§è¡Œ|è°ƒç”¨äº†)\s*.{0,10}(å‘½ä»¤|æŒ‡ä»¤|å·¥å…·|rm|touch|mkdir|cp|mv|cat|git|pip|npm|python)", _re.IGNORECASE),
    # 3. å£°ç§°æ“ä½œç»“æœ: "å·²æˆåŠŸåˆ é™¤/åˆ›å»º/..." æˆ– "æ–‡ä»¶å·²åˆ é™¤"
    _re.compile(r"(å·²|å·²ç»|å·²æˆåŠŸ|æˆåŠŸ)(åˆ é™¤|åˆ›å»º|ç§»åŠ¨|å¤åˆ¶|ä¿®æ”¹|ç§»é™¤|å®‰è£…|å¸è½½|åœæ­¢|å¯åŠ¨|é‡å¯|å†™å…¥|æ¸…é™¤|æ¸…ç©º)", _re.IGNORECASE),
    # 4. æè¿°å‘½ä»¤è¾“å‡º/ç»“æœ
    _re.compile(r"(å‘½ä»¤|æŒ‡ä»¤).{0,20}(æ‰§è¡Œ|è¿è¡Œ).{0,10}(å®Œæˆ|æˆåŠŸ|å®Œæ¯•|ç»“æœ|è¾“å‡º|æ˜¾ç¤º)", _re.IGNORECASE),
    # 5. å£°ç§°æ–‡ä»¶/ç›®å½•çŠ¶æ€å˜åŒ–
    _re.compile(r"(æ–‡ä»¶|ç›®å½•|æ–‡ä»¶å¤¹).{0,30}(ä¸å­˜åœ¨|å·²è¢«åˆ é™¤|å·²åˆ é™¤|å·²åˆ›å»º|å·²ç§»åŠ¨|å·²è¢«ç§»é™¤|å·²æ¸…ç©º|å·²è¢«æ¸…ç©º)", _re.IGNORECASE),
    # 6. è·¯å¾„ + çŠ¶æ€æè¿° (å¦‚ "/tmp/5.txt æ–‡ä»¶ä¸å­˜åœ¨")
    _re.compile(r"/\S+\s+(æ–‡ä»¶|ç›®å½•|æ–‡ä»¶å¤¹)?.{0,5}(ä¸å­˜åœ¨|å·²è¢«?åˆ é™¤|å·²è¢«?ç§»é™¤|å·²åˆ›å»º)", _re.IGNORECASE),
    # 7. å£°ç§°é€šè¿‡å·¥å…·/tool è°ƒç”¨
    _re.compile(r"(é€šè¿‡|ä½¿ç”¨|åˆ©ç”¨).{0,5}(å·¥å…·|tool).{0,10}(è°ƒç”¨|æ‰§è¡Œ|è¿è¡Œ)", _re.IGNORECASE),
    # 8. å‘½ä»¤æ‰§è¡Œç»“æœæ˜¾ç¤º (narrating output)
    _re.compile(r"(æ‰§è¡Œç»“æœ|è¾“å‡ºç»“æœ|è¿”å›ç»“æœ|è¿è¡Œç»“æœ|ç»“æœæ˜¾ç¤º|è¾“å‡ºæ˜¾ç¤º|ç»“æœå¦‚ä¸‹|è¾“å‡ºå¦‚ä¸‹)", _re.IGNORECASE),
    # 9. å£°ç§° "No such file" ç­‰è‹±æ–‡å‘½ä»¤è¾“å‡º (åœ¨æ—  tool call æ—¶ä¸åº”å‡ºç°)
    _re.compile(r"(No such file|Permission denied|command not found|cannot remove|cannot create|Operation not permitted)", _re.IGNORECASE),
]


def _detect_fabrication(text: str) -> bool:
    """
    æ£€æµ‹æ¨¡å‹æ˜¯å¦åœ¨æ–‡æœ¬ä¸­ä¼ªé€ äº†å·¥å…·æ‰§è¡Œç»“æœã€‚

    ç­–ç•¥: å®½æ¾åŒ¹é…ï¼Œå®å¯è¯¯åˆ¤ï¼ˆæœ€å¤šå¤šä¸€æ¬¡é‡è¯•ï¼‰ä¹Ÿä¸æ”¾è¿‡ä¼ªé€ ã€‚
    å½“æ¨¡å‹è¾“å‡ºäº†ç±»ä¼¼ "å·²é€šè¿‡å·¥å…·è°ƒç”¨æ‰§è¡Œ rm /tmp/3.txt" è¿™æ ·çš„å†…å®¹ï¼Œ
    ä½†å®é™…ä¸Šæ²¡æœ‰å‘èµ·ä»»ä½• tool_call æ—¶ï¼Œåˆ¤å®šä¸ºä¼ªé€ ã€‚
    """
    if not text or len(text) < 10:
        return False
    for pattern in _FABRICATION_PATTERNS:
        if pattern.search(text):
            logger.info(f"ä¼ªé€ æ£€æµ‹å‘½ä¸­æ¨¡å¼: {pattern.pattern[:50]}... | æ–‡æœ¬ç‰‡æ®µ: {text[:100]}")
            return True
    return False


# ==================== AI chat_stream ====================

# å·¥å…·è°ƒç”¨æœ€å¤§å¾ªç¯æ¬¡æ•° (é˜²æ­¢æ— é™ tool-call å¾ªç¯)
# åŒä¸€ request_id ä¸‹çš„å·¥å…·è½®æ¬¡åº”å½’é›†ä¸ºä¸€æ¬¡ premium request
MAX_TOOL_ROUNDS = 15

# Tool call æ‰§è¡Œå›è°ƒç±»å‹
ToolExecutor = Callable[[str, Dict[str, Any]], Awaitable[str]]


async def chat_stream(
    messages: List[Dict[str, Any]],
    model: str = "gpt-4o",
    system_prompt: str = "",
    temperature: float = 0.7,
    max_tokens: int = 8192,
    tools: Optional[List[Dict[str, Any]]] = None,
    tool_executor: Optional[ToolExecutor] = None,
    request_id: str = "",
    max_tool_rounds: int = MAX_TOOL_ROUNDS,
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æµå¼ AI å¯¹è¯ (SSE) â€” ç»“æ„åŒ–è¾“å‡º, æ”¯æŒ Tool Calling

    yield å­—å…¸ç±»å‹:
      {"type": "content",    "content": "..."}                                    - æ–‡æœ¬å†…å®¹å¢é‡
      {"type": "thinking",   "content": "..."}                                    - æ€è€ƒè¿‡ç¨‹å¢é‡
      {"type": "tool_call",  "tool_call": {"id":"..","name":"..","arguments":{..}}} - AI è¯·æ±‚è°ƒç”¨å·¥å…·
      {"type": "tool_result","tool_call_id":"..","name":"..","result":"..","duration_ms":N} - å·¥å…·ç»“æœ
      {"type": "tool_error", "tool_call_id":"..","name":"..","error":".."}         - å·¥å…·é”™è¯¯
      {"type": "usage",      "usage": {...}}                                      - token ä½¿ç”¨ç»Ÿè®¡
      {"type": "error",      "error": "..."}                                      - é”™è¯¯ä¿¡æ¯
    """
    use_copilot = _is_copilot_model(model)
    actual_model = _get_actual_model_name(model)
    is_reasoning = _is_reasoning_model(actual_model)

    # è§£ææä¾›å•†ä¿¡æ¯
    provider = await _resolve_provider(model)
    actual_model = provider.actual_model
    is_reasoning = _is_reasoning_model(actual_model)

    # æ¨ç†æ¨¡å‹ä¸æ”¯æŒ tools, å¼ºåˆ¶ç¦ç”¨
    if is_reasoning and tools:
        logger.info(f"æ¨ç†æ¨¡å‹ {actual_model} ä¸æ”¯æŒ tools, è·³è¿‡å·¥å…·æ³¨å…¥")
        tools = None

    # éªŒè¯è®¤è¯
    if provider.provider_type == "copilot":
        if not copilot_auth.is_authenticated:
            yield {"type": "error", "error": "âŒ æœªæˆæƒ Copilotï¼Œè¯·åœ¨è®¾ç½®é¡µé¢å®Œæˆ OAuth æˆæƒ"}
            return
    elif provider.provider_type == "github_models":
        if not settings.github_token:
            yield {"type": "error", "error": "âŒ æœªé…ç½® GITHUB_TOKENï¼Œæ— æ³•è°ƒç”¨ AI æœåŠ¡"}
            return
    elif provider.provider_type == "openai_compatible":
        if not provider.api_key:
            yield {"type": "error", "error": f"âŒ {provider.name} æœªé…ç½® API Keyï¼Œè¯·åœ¨ AI æœåŠ¡è®¾ç½®ä¸­é…ç½®"}
            return

    # å·¥å…·è°ƒç”¨å¾ªç¯ â€” æ¨¡å‹å¯èƒ½å¤šæ¬¡è°ƒç”¨å·¥å…·
    current_messages = list(messages)  # å¯è¿½åŠ  tool results
    total_tool_rounds = 0
    seen_tool_calls: set = set()  # æ£€æµ‹é‡å¤è°ƒç”¨ (name + args hash)
    all_tool_calls_collected: List[Dict[str, Any]] = []  # æ”¶é›†æ‰€æœ‰è½®æ¬¡çš„ tool calls
    fabrication_retries = 0  # ä¼ªé€ æ£€æµ‹é‡è¯•è®¡æ•°

    while True:
        # æ„å»ºæ¶ˆæ¯
        api_messages = _build_api_messages(current_messages, system_prompt, is_reasoning)

        # è·å–è¯·æ±‚å¤´å’Œ URL
        if provider.provider_type == "copilot":
            try:
                headers = await _get_copilot_headers(request_id=request_id)
            except Exception as e:
                yield {"type": "error", "error": f"âŒ Copilot è®¤è¯å¤±è´¥: {str(e)}"}
                return
            base_url = provider.base_url
            logger.info(f"Using Copilot API for model: {actual_model} (request_id: {request_id[:8]}...)" if request_id else f"Using Copilot API for model: {actual_model}")
        elif provider.provider_type == "openai_compatible":
            headers = {
                "Authorization": f"Bearer {provider.api_key}",
                "Content-Type": "application/json",
            }
            base_url = provider.base_url.rstrip("/")
            logger.info(f"Using {provider.name} ({provider.slug}) for model: {actual_model}")
        else:
            headers = _get_models_headers()
            base_url = provider.base_url
            logger.info(f"Using GitHub Models API for model: {actual_model}")

        # æ„å»º payload
        if is_reasoning:
            payload: Dict[str, Any] = {
                "model": actual_model,
                "messages": api_messages,
                "max_completion_tokens": max_tokens,
            }
            logger.info(f"Using reasoning model params for {actual_model}")
        else:
            payload = {
                "model": actual_model,
                "messages": api_messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True,
            }

        # æ³¨å…¥ tools
        if tools:
            payload["tools"] = tools
            # ä¼ªé€ é‡è¯•æ—¶å¼ºåˆ¶æ¨¡å‹å¿…é¡»è°ƒç”¨å·¥å…·
            if fabrication_retries > 0:
                payload["tool_choice"] = "required"
                logger.info(f"ä¼ªé€ é‡è¯•ä¸­, å¼ºåˆ¶ tool_choice=required (retry={fabrication_retries})")
            else:
                payload["tool_choice"] = "auto"

        try:
            async with httpx.AsyncClient(timeout=300) as client:
                if is_reasoning:
                    # æ¨ç†æ¨¡å‹ä¸æ”¯æŒæµå¼
                    response = await client.post(
                        f"{base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                    )
                    if response.status_code != 200:
                        error_text = response.text
                        logger.error(f"AI API error {response.status_code}: {error_text}")
                        capability_cache.learn_from_error(model, error_text)
                        error_meta = _parse_error_metadata(response.status_code, error_text, model)
                        yield {"type": "error", "error": f"âŒ AI æœåŠ¡é”™è¯¯ ({response.status_code}): {error_text}", "error_meta": error_meta}
                        return

                    result = response.json()
                    choice = result.get("choices", [{}])[0]
                    message_data = choice.get("message", {})

                    # æ€è€ƒè¿‡ç¨‹
                    thinking = message_data.get("reasoning_content") or message_data.get("thinking") or ""
                    if thinking:
                        yield {"type": "thinking", "content": thinking}

                    # ä¸»è¦å†…å®¹
                    content = message_data.get("content", "")
                    if content:
                        yield {"type": "content", "content": content}

                    # token ä½¿ç”¨ç»Ÿè®¡
                    usage = result.get("usage")
                    if usage:
                        yield {"type": "usage", "usage": {
                            "prompt_tokens": usage.get("prompt_tokens", 0),
                            "completion_tokens": usage.get("completion_tokens", 0),
                            "total_tokens": usage.get("total_tokens", 0),
                            "reasoning_tokens": usage.get("completion_tokens_details", {}).get("reasoning_tokens", 0),
                        }}
                    # æ¨ç†æ¨¡å‹ä¸èµ°å¾ªç¯
                    return

                else:
                    # æ™®é€šæ¨¡å‹ä½¿ç”¨æµå¼
                    async with client.stream(
                        "POST",
                        f"{base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                    ) as response:
                        if response.status_code != 200:
                            error_body = await response.aread()
                            error_text = error_body.decode()
                            logger.error(f"AI API error {response.status_code}: {error_text}")
                            capability_cache.learn_from_error(model, error_text)
                            error_meta = _parse_error_metadata(response.status_code, error_text, model)
                            yield {"type": "error", "error": f"âŒ AI æœåŠ¡é”™è¯¯ ({response.status_code}): {error_text}", "error_meta": error_meta}
                            return

                        usage_data = None
                        # tool_calls ç´¯ç§¯å™¨ (æµå¼ä¸­åˆ†ç‰‡åˆ°è¾¾)
                        pending_tool_calls: Dict[int, Dict[str, Any]] = {}
                        started_tool_calls: set = set()  # å·²å‘é€ tool_call_start çš„ idx
                        response_has_content = False
                        stream_finish_reason = None  # è·Ÿè¸ªæµç»“æŸåŸå› 
                        response_text_parts: List[str] = []  # æ”¶é›†å®Œæ•´æ–‡æœ¬ç”¨äºä¼ªé€ æ£€æµ‹

                        async for line in response.aiter_lines():
                            if line.startswith("data: "):
                                data = line[6:]
                                if data.strip() == "[DONE]":
                                    break
                                try:
                                    chunk = json.loads(data)
                                    choice0 = chunk.get("choices", [{}])[0]
                                    delta = choice0.get("delta", {})

                                    # è·Ÿè¸ª finish_reason
                                    fr = choice0.get("finish_reason")
                                    if fr:
                                        stream_finish_reason = fr

                                    # æ€è€ƒè¿‡ç¨‹ (æµå¼, Claude ç­‰æ¨¡å‹)
                                    thinking_delta = delta.get("reasoning_content") or delta.get("thinking") or ""
                                    if thinking_delta:
                                        yield {"type": "thinking", "content": thinking_delta}

                                    # ä¸»è¦å†…å®¹
                                    if "content" in delta and delta["content"]:
                                        yield {"type": "content", "content": delta["content"]}
                                        response_has_content = True
                                        response_text_parts.append(delta["content"])

                                    # Tool calls å¢é‡ç´¯ç§¯
                                    if "tool_calls" in delta:
                                        for tc_delta in delta["tool_calls"]:
                                            idx = tc_delta.get("index", 0)
                                            if idx not in pending_tool_calls:
                                                pending_tool_calls[idx] = {
                                                    "id": tc_delta.get("id", ""),
                                                    "name": "",
                                                    "arguments": "",
                                                }
                                            tc = pending_tool_calls[idx]
                                            if tc_delta.get("id"):
                                                tc["id"] = tc_delta["id"]
                                            func = tc_delta.get("function", {})
                                            if func.get("name"):
                                                tc["name"] = func["name"]
                                                # ask_user æå‰é€šçŸ¥: è®©å‰ç«¯å°½æ—©æ˜¾ç¤º loading å¡ç‰‡
                                                if func["name"] == "ask_user" and idx not in started_tool_calls and tc["id"]:
                                                    started_tool_calls.add(idx)
                                                    yield {"type": "tool_call_start", "tool_call": {"id": tc["id"], "name": "ask_user"}}
                                            if func.get("arguments"):
                                                tc["arguments"] += func["arguments"]

                                    # æµå¼ usage
                                    chunk_usage = chunk.get("usage")
                                    if chunk_usage:
                                        usage_data = chunk_usage

                                except Exception:
                                    continue

                        # å‘é€ usage (ä¸ç®¡æ˜¯å¦æœ‰ tool calls)
                        if usage_data:
                            yield {"type": "usage", "usage": {
                                "prompt_tokens": usage_data.get("prompt_tokens", 0),
                                "completion_tokens": usage_data.get("completion_tokens", 0),
                                "total_tokens": usage_data.get("total_tokens", 0),
                                "tool_rounds": total_tool_rounds,
                            }}

                        # æ£€æµ‹è¾“å‡ºè¢«æˆªæ–­ (max_tokens è€—å°½)
                        if stream_finish_reason == "length":
                            if pending_tool_calls:
                                # å·¥å…·è°ƒç”¨å‚æ•°è¢«æˆªæ–­ (ä¸å®Œæ•´), ä¸¢å¼ƒå¹¶æ ‡è®°æˆªæ–­
                                logger.info(f"æ¨¡å‹ {actual_model} è¾“å‡ºå›  max_tokens æˆªæ–­ (finish_reason=length), ä¸¢å¼ƒ {len(pending_tool_calls)} ä¸ªä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨")
                                pending_tool_calls.clear()
                            if response_has_content:
                                logger.info(f"æ¨¡å‹ {actual_model} è¾“å‡ºå›  max_tokens æˆªæ–­ (finish_reason=length)")
                                yield {"type": "truncated"}

                        # æ£€æŸ¥æ˜¯å¦æœ‰ tool calls éœ€è¦æ‰§è¡Œ
                        if pending_tool_calls and tool_executor:
                            total_tool_rounds += 1
                            if total_tool_rounds > max_tool_rounds:
                                yield {"type": "content", "content": "\n\nâš ï¸ å·¥å…·è°ƒç”¨å·²è¾¾ä¸Šé™ ({}è½®)ï¼Œåœæ­¢ç»§ç»­è°ƒç”¨ã€‚".format(max_tool_rounds)}
                                return

                            # è§£æå¹¶æ‰§è¡Œ tool calls
                            sorted_tcs = sorted(pending_tool_calls.items())
                            tool_results_messages = []

                            # å…ˆè¿½åŠ  assistant çš„ tool_calls æ¶ˆæ¯ (OpenAI åè®®è¦æ±‚)
                            assistant_tool_calls = []
                            for _, tc in sorted_tcs:
                                assistant_tool_calls.append({
                                    "id": tc["id"],
                                    "type": "function",
                                    "function": {
                                        "name": tc["name"],
                                        "arguments": tc["arguments"],
                                    },
                                })
                            current_messages.append({
                                "role": "assistant",
                                "content": None,
                                "tool_calls": assistant_tool_calls,
                            })

                            for _, tc in sorted_tcs:
                                try:
                                    arguments = json.loads(tc["arguments"]) if tc["arguments"] else {}
                                except json.JSONDecodeError:
                                    arguments = {"_raw": tc["arguments"]}

                                # æ£€æµ‹é‡å¤å·¥å…·è°ƒç”¨ (é˜²æ­¢æ¨¡å‹é™·å…¥å¾ªç¯)
                                call_sig = f"{tc['name']}:{json.dumps(arguments, sort_keys=True)}"
                                is_duplicate = call_sig in seen_tool_calls
                                seen_tool_calls.add(call_sig)

                                # yield tool_call äº‹ä»¶
                                yield {
                                    "type": "tool_call",
                                    "tool_call": {
                                        "id": tc["id"],
                                        "name": tc["name"],
                                        "arguments": arguments,
                                    },
                                }

                                # é‡å¤è°ƒç”¨: ç›´æ¥è¿”å›æç¤º, ä¸å®é™…æ‰§è¡Œ
                                if is_duplicate:
                                    result_text = "âš ï¸ ä½ å·²ç»è¯»å–è¿‡è¿™ä¸ªå†…å®¹äº†ï¼Œè¯·ç›´æ¥ä½¿ç”¨ä¹‹å‰çš„ç»“æœï¼Œä¸è¦é‡å¤è¯»å–ã€‚"
                                    yield {
                                        "type": "tool_result",
                                        "tool_call_id": tc["id"],
                                        "name": tc["name"],
                                        "arguments": arguments,
                                        "result": result_text,
                                        "duration_ms": 0,
                                    }
                                    tool_results_messages.append({
                                        "role": "tool",
                                        "tool_call_id": tc["id"],
                                        "content": result_text,
                                    })
                                    continue

                                # æ‰§è¡Œå·¥å…·
                                start_time = time.monotonic()
                                try:
                                    result_text = await tool_executor(tc["name"], arguments)
                                    duration_ms = int((time.monotonic() - start_time) * 1000)

                                    # æˆªæ–­å·¥å…·ç»“æœä»¥é€‚é…å°ä¸Šä¸‹æ–‡æ¨¡å‹
                                    max_input, _ = capability_cache.get_context_window(model)
                                    current_tokens = estimate_messages_tokens(current_messages)
                                    result_tokens = estimate_tokens(result_text)
                                    # ç»™æ¨¡å‹å›å¤å’Œå…¶ä»– tool calls ç•™å‡ºç©ºé—´
                                    remaining_budget = max_input - current_tokens - max_tokens - 200
                                    if result_tokens > remaining_budget and remaining_budget > 500:
                                        result_text = truncate_text(result_text, remaining_budget)
                                        result_text += f"\n\n[â€¦ å†…å®¹å·²æˆªæ–­ä»¥é€‚é…æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ ({remaining_budget} tokens), è¯·ç”¨ start_line/end_line æŒ‡å®šèŒƒå›´ç²¾ç¡®è¯»å–]"
                                        logger.info(f"å·¥å…·ç»“æœæˆªæ–­: {result_tokens} -> {remaining_budget} tokens (model={model}, budget={max_input})")
                                    elif remaining_budget <= 500:
                                        # ä¸Šä¸‹æ–‡å·²ç»æåº¦ç´§å¼ , åªä¿ç•™æ‘˜è¦
                                        result_text = truncate_text(result_text, 500)
                                        result_text += "\n\n[âš ï¸ ä¸Šä¸‹æ–‡ç©ºé—´ä¸è¶³, å†…å®¹å·²å¤§å¹…æˆªæ–­. å»ºè®®: ç”¨ start_line/end_line æŒ‡å®šå°èŒƒå›´, æˆ–åˆ‡æ¢åˆ°æ›´å¤§ä¸Šä¸‹æ–‡çš„æ¨¡å‹]"
                                        logger.warning(f"ä¸Šä¸‹æ–‡æåº¦ç´§å¼ , å·¥å…·ç»“æœå¼ºåˆ¶æˆªæ–­åˆ° 500 tokens (model={model})")

                                    yield {
                                        "type": "tool_result",
                                        "tool_call_id": tc["id"],
                                        "name": tc["name"],
                                        "arguments": arguments,
                                        "result": result_text,
                                        "duration_ms": duration_ms,
                                    }

                                    # è®°å½•å®Œæ•´çš„ tool call ä¿¡æ¯
                                    all_tool_calls_collected.append({
                                        "id": tc["id"],
                                        "name": tc["name"],
                                        "arguments": arguments,
                                        "result": result_text,
                                        "duration_ms": duration_ms,
                                    })

                                    tool_results_messages.append({
                                        "role": "tool",
                                        "tool_call_id": tc["id"],
                                        "content": result_text,
                                    })

                                except Exception as e:
                                    duration_ms = int((time.monotonic() - start_time) * 1000)
                                    error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                                    yield {
                                        "type": "tool_error",
                                        "tool_call_id": tc["id"],
                                        "name": tc["name"],
                                        "error": error_msg,
                                    }
                                    all_tool_calls_collected.append({
                                        "id": tc["id"],
                                        "name": tc["name"],
                                        "arguments": arguments,
                                        "result": f"ERROR: {error_msg}",
                                        "duration_ms": duration_ms,
                                    })
                                    tool_results_messages.append({
                                        "role": "tool",
                                        "tool_call_id": tc["id"],
                                        "content": error_msg,
                                    })

                            # è¿½åŠ  tool results åˆ°æ¶ˆæ¯åˆ—è¡¨
                            current_messages.extend(tool_results_messages)

                            # å¦‚æœæœ¬è½®è°ƒç”¨äº† ask_userï¼Œä¸­æ–­å¾ªç¯ï¼Œç­‰å¾…ç”¨æˆ·å›ç­”
                            has_ask_user = any(tc["name"] == "ask_user" for _, tc in sorted_tcs)
                            if has_ask_user:
                                yield {"type": "ask_user_pending"}
                                # ä¸ continue â€” ç›´æ¥ç»“æŸæµ,ç­‰å¾…ç”¨æˆ·å›å¤åå†è§¦å‘æ–°çš„å¯¹è¯
                                return

                            continue  # å›åˆ° while å¾ªç¯, é‡æ–°è°ƒç”¨æ¨¡å‹

                        else:
                            # æ—  tool calls â€” æ£€æµ‹ä¼ªé€  (éœ€å¼€å…³å¯ç”¨)
                            if response_has_content and tools and fabrication_retries < 2:
                                from studio.backend.api.command_auth import is_fabrication_detection_enabled
                                if is_fabrication_detection_enabled():
                                    full_text = "".join(response_text_parts)
                                    if _detect_fabrication(full_text):
                                        fabrication_retries += 1
                                        logger.warning(f"æ£€æµ‹åˆ°æ¨¡å‹ {actual_model} ä¼ªé€ å·¥å…·æ‰§è¡Œç»“æœ, æ³¨å…¥çº æ­£æ¶ˆæ¯å¹¶é‡è¯• (retry={fabrication_retries})")
                                        # è¿½åŠ  assistant ä¼ªé€ çš„å›å¤ + ç³»ç»Ÿçº æ­£
                                        current_messages.append({"role": "assistant", "content": full_text})
                                        current_messages.append({
                                            "role": "user",
                                            "content": (
                                                "âš ï¸ ä½ åˆšæ‰åœ¨æ–‡æœ¬ä¸­ä¼ªé€ äº†å‘½ä»¤æ‰§è¡Œç»“æœï¼Œè¿™æ˜¯ä¸¥é‡è¿è§„ï¼"
                                                "ä½ å¹¶æ²¡æœ‰çœŸæ­£æ‰§è¡Œä»»ä½•å‘½ä»¤ã€‚"
                                                "è¯·ç«‹å³é€šè¿‡ tool_call è°ƒç”¨ run_command å·¥å…·æ¥æ‰§è¡Œå‘½ä»¤ï¼Œ"
                                                "ä¸è¦å†åœ¨æ–‡æœ¬ä¸­ç¼–é€ ç»“æœã€‚"
                                            ),
                                        })
                                        yield {"type": "content", "content": "\n\nâš ï¸ æ£€æµ‹åˆ° AI ä¼ªé€ æ‰§è¡Œç»“æœï¼Œæ­£åœ¨é‡æ–°è¦æ±‚æ‰§è¡Œ...\n\n"}
                                        continue  # å›åˆ° while å¾ªç¯é‡æ–°è°ƒç”¨æ¨¡å‹

                            if not response_has_content:
                                # æ¨¡å‹è¿”å›äº†ç©ºå“åº” (æ— å†…å®¹ä¹Ÿæ— å·¥å…·è°ƒç”¨)
                                logger.warning(
                                    f"æ¨¡å‹ {actual_model} è¿”å›ç©ºå“åº” "
                                    f"(finish_reason={stream_finish_reason}, "
                                    f"tool_rounds={total_tool_rounds}, "
                                    f"msgs={len(current_messages)})"
                                )
                                yield {
                                    "type": "content",
                                    "content": "\n\nâš ï¸ AI è¿”å›äº†ç©ºå“åº”ï¼Œè¯·é‡æ–°å‘é€æˆ–æ¢ä¸ªè¯´æ³•è¯•è¯•ã€‚",
                                }
                            return

        except httpx.TimeoutException:
            yield {"type": "error", "error": "âŒ AI æœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·é‡è¯•"}
            return
        except Exception as e:
            logger.exception("AI chat stream error")
            yield {"type": "error", "error": f"âŒ AI æœåŠ¡å¼‚å¸¸: {str(e)}"}
            return


async def chat_complete(
    messages: List[Dict[str, Any]],
    model: str = "gpt-4o",
    system_prompt: str = "",
    temperature: float = 0.7,
    max_tokens: int = 8192,
) -> str:
    """åŒæ­¥ AI å¯¹è¯ (éæµå¼, ç”¨äºç”Ÿæˆ plan ç­‰) â€” åªè¿”å› content æ–‡æœ¬"""
    result = []
    async for event in chat_stream(messages, model, system_prompt, temperature, max_tokens):
        if isinstance(event, dict):
            if event.get("type") == "content":
                result.append(event["content"])
            elif event.get("type") == "error":
                result.append(event["error"])
        else:
            result.append(str(event))
    return "".join(result)


def encode_image_to_base64(file_bytes: bytes) -> str:
    """å°†å›¾ç‰‡å­—èŠ‚ç¼–ç ä¸º base64"""
    return base64.b64encode(file_bytes).decode("utf-8")


def get_mime_type(filename: str) -> str:
    """æ ¹æ®æ–‡ä»¶åè·å– MIME ç±»å‹"""
    mime_type, _ = mimetypes.guess_type(filename)
    return mime_type or "image/png"
